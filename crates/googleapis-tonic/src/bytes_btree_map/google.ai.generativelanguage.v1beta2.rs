// This file is @generated by prost-build.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CitationMetadata {
#[prost(message, repeated, tag = "1")]
pub citation_sources: ::prost::alloc::vec::Vec<CitationSource>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CitationSource {
#[prost(int32, optional, tag = "1")]
pub start_index: ::core::option::Option<i32>,
#[prost(int32, optional, tag = "2")]
pub end_index: ::core::option::Option<i32>,
#[prost(string, optional, tag = "3")]
pub uri: ::core::option::Option<::prost::alloc::string::String>,
#[prost(string, optional, tag = "4")]
pub license: ::core::option::Option<::prost::alloc::string::String>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ContentFilter {
#[prost(enumeration = "content_filter::BlockedReason", tag = "1")]
pub reason: i32,
#[prost(string, optional, tag = "2")]
pub message: ::core::option::Option<::prost::alloc::string::String>,
}
/// Nested message and enum types in `ContentFilter`.
pub mod content_filter {
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum BlockedReason {
Unspecified = 0,
Safety = 1,
Other = 2,
}
impl BlockedReason {
/// String value of the enum field names used in the ProtoBuf definition.
///
/// The values are not transformed in any way and thus are considered stable
/// (if the ProtoBuf definition does not change) and safe for programmatic use.
pub fn as_str_name(&self) -> &'static str {
match self {
BlockedReason::Unspecified => "BLOCKED_REASON_UNSPECIFIED",
BlockedReason::Safety => "SAFETY",
BlockedReason::Other => "OTHER",
}
}
/// Creates an enum from field names used in the ProtoBuf definition.
pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
match value {
"BLOCKED_REASON_UNSPECIFIED" => Some(Self::Unspecified),
"SAFETY" => Some(Self::Safety),
"OTHER" => Some(Self::Other),
_ => None,
}
}
}
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct SafetyFeedback {
#[prost(message, optional, tag = "1")]
pub rating: ::core::option::Option<SafetyRating>,
#[prost(message, optional, tag = "2")]
pub setting: ::core::option::Option<SafetySetting>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct SafetyRating {
#[prost(enumeration = "HarmCategory", tag = "3")]
pub category: i32,
#[prost(enumeration = "safety_rating::HarmProbability", tag = "4")]
pub probability: i32,
}
/// Nested message and enum types in `SafetyRating`.
pub mod safety_rating {
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum HarmProbability {
Unspecified = 0,
Negligible = 1,
Low = 2,
Medium = 3,
High = 4,
}
impl HarmProbability {
/// String value of the enum field names used in the ProtoBuf definition.
///
/// The values are not transformed in any way and thus are considered stable
/// (if the ProtoBuf definition does not change) and safe for programmatic use.
pub fn as_str_name(&self) -> &'static str {
match self {
HarmProbability::Unspecified => "HARM_PROBABILITY_UNSPECIFIED",
HarmProbability::Negligible => "NEGLIGIBLE",
HarmProbability::Low => "LOW",
HarmProbability::Medium => "MEDIUM",
HarmProbability::High => "HIGH",
}
}
/// Creates an enum from field names used in the ProtoBuf definition.
pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
match value {
"HARM_PROBABILITY_UNSPECIFIED" => Some(Self::Unspecified),
"NEGLIGIBLE" => Some(Self::Negligible),
"LOW" => Some(Self::Low),
"MEDIUM" => Some(Self::Medium),
"HIGH" => Some(Self::High),
_ => None,
}
}
}
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct SafetySetting {
#[prost(enumeration = "HarmCategory", tag = "3")]
pub category: i32,
#[prost(enumeration = "safety_setting::HarmBlockThreshold", tag = "4")]
pub threshold: i32,
}
/// Nested message and enum types in `SafetySetting`.
pub mod safety_setting {
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum HarmBlockThreshold {
Unspecified = 0,
BlockLowAndAbove = 1,
BlockMediumAndAbove = 2,
BlockOnlyHigh = 3,
}
impl HarmBlockThreshold {
/// String value of the enum field names used in the ProtoBuf definition.
///
/// The values are not transformed in any way and thus are considered stable
/// (if the ProtoBuf definition does not change) and safe for programmatic use.
pub fn as_str_name(&self) -> &'static str {
match self {
HarmBlockThreshold::Unspecified => "HARM_BLOCK_THRESHOLD_UNSPECIFIED",
HarmBlockThreshold::BlockLowAndAbove => "BLOCK_LOW_AND_ABOVE",
HarmBlockThreshold::BlockMediumAndAbove => "BLOCK_MEDIUM_AND_ABOVE",
HarmBlockThreshold::BlockOnlyHigh => "BLOCK_ONLY_HIGH",
}
}
/// Creates an enum from field names used in the ProtoBuf definition.
pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
match value {
"HARM_BLOCK_THRESHOLD_UNSPECIFIED" => Some(Self::Unspecified),
"BLOCK_LOW_AND_ABOVE" => Some(Self::BlockLowAndAbove),
"BLOCK_MEDIUM_AND_ABOVE" => Some(Self::BlockMediumAndAbove),
"BLOCK_ONLY_HIGH" => Some(Self::BlockOnlyHigh),
_ => None,
}
}
}
}
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum HarmCategory {
Unspecified = 0,
Derogatory = 1,
Toxicity = 2,
Violence = 3,
Sexual = 4,
Medical = 5,
Dangerous = 6,
}
impl HarmCategory {
/// String value of the enum field names used in the ProtoBuf definition.
///
/// The values are not transformed in any way and thus are considered stable
/// (if the ProtoBuf definition does not change) and safe for programmatic use.
pub fn as_str_name(&self) -> &'static str {
match self {
HarmCategory::Unspecified => "HARM_CATEGORY_UNSPECIFIED",
HarmCategory::Derogatory => "HARM_CATEGORY_DEROGATORY",
HarmCategory::Toxicity => "HARM_CATEGORY_TOXICITY",
HarmCategory::Violence => "HARM_CATEGORY_VIOLENCE",
HarmCategory::Sexual => "HARM_CATEGORY_SEXUAL",
HarmCategory::Medical => "HARM_CATEGORY_MEDICAL",
HarmCategory::Dangerous => "HARM_CATEGORY_DANGEROUS",
}
}
/// Creates an enum from field names used in the ProtoBuf definition.
pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
match value {
"HARM_CATEGORY_UNSPECIFIED" => Some(Self::Unspecified),
"HARM_CATEGORY_DEROGATORY" => Some(Self::Derogatory),
"HARM_CATEGORY_TOXICITY" => Some(Self::Toxicity),
"HARM_CATEGORY_VIOLENCE" => Some(Self::Violence),
"HARM_CATEGORY_SEXUAL" => Some(Self::Sexual),
"HARM_CATEGORY_MEDICAL" => Some(Self::Medical),
"HARM_CATEGORY_DANGEROUS" => Some(Self::Dangerous),
_ => None,
}
}
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GenerateTextRequest {
#[prost(string, tag = "1")]
pub model: ::prost::alloc::string::String,
#[prost(message, optional, tag = "2")]
pub prompt: ::core::option::Option<TextPrompt>,
#[prost(float, optional, tag = "3")]
pub temperature: ::core::option::Option<f32>,
#[prost(int32, optional, tag = "4")]
pub candidate_count: ::core::option::Option<i32>,
#[prost(int32, optional, tag = "5")]
pub max_output_tokens: ::core::option::Option<i32>,
#[prost(float, optional, tag = "6")]
pub top_p: ::core::option::Option<f32>,
#[prost(int32, optional, tag = "7")]
pub top_k: ::core::option::Option<i32>,
#[prost(message, repeated, tag = "8")]
pub safety_settings: ::prost::alloc::vec::Vec<SafetySetting>,
#[prost(string, repeated, tag = "9")]
pub stop_sequences: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GenerateTextResponse {
#[prost(message, repeated, tag = "1")]
pub candidates: ::prost::alloc::vec::Vec<TextCompletion>,
#[prost(message, repeated, tag = "3")]
pub filters: ::prost::alloc::vec::Vec<ContentFilter>,
#[prost(message, repeated, tag = "4")]
pub safety_feedback: ::prost::alloc::vec::Vec<SafetyFeedback>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TextPrompt {
#[prost(string, tag = "1")]
pub text: ::prost::alloc::string::String,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TextCompletion {
#[prost(string, tag = "1")]
pub output: ::prost::alloc::string::String,
#[prost(message, repeated, tag = "2")]
pub safety_ratings: ::prost::alloc::vec::Vec<SafetyRating>,
#[prost(message, optional, tag = "3")]
pub citation_metadata: ::core::option::Option<CitationMetadata>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct EmbedTextRequest {
#[prost(string, tag = "1")]
pub model: ::prost::alloc::string::String,
#[prost(string, tag = "2")]
pub text: ::prost::alloc::string::String,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct EmbedTextResponse {
#[prost(message, optional, tag = "1")]
pub embedding: ::core::option::Option<Embedding>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Embedding {
#[prost(float, repeated, tag = "1")]
pub value: ::prost::alloc::vec::Vec<f32>,
}
/// Generated client implementations.
pub mod text_service_client {
#![allow(unused_variables, dead_code, missing_docs, clippy::let_unit_value)]
use tonic::codegen::http::Uri;
use tonic::codegen::*;
/// API for using Generative Language Models (GLMs) trained to generate text.
///
/// Also known as Large Language Models (LLM)s, these generate text given an
/// input prompt from the user.
#[derive(Debug, Clone)]
pub struct TextServiceClient<T> {
inner: tonic::client::Grpc<T>,
}
impl<T> TextServiceClient<T>
where
T: tonic::client::GrpcService<tonic::body::BoxBody>,
T::Error: Into<StdError>,
T::ResponseBody: Body<Data = Bytes> + Send + 'static,
<T::ResponseBody as Body>::Error: Into<StdError> + Send,
{
pub fn new(inner: T) -> Self {
let inner = tonic::client::Grpc::new(inner);
Self { inner }
}
pub fn with_origin(inner: T, origin: Uri) -> Self {
let inner = tonic::client::Grpc::with_origin(inner, origin);
Self { inner }
}
pub fn with_interceptor<F>(inner: T, interceptor: F) -> TextServiceClient<InterceptedService<T, F>>
where
F: tonic::service::Interceptor,
T::ResponseBody: Default,
T: tonic::codegen::Service<http::Request<tonic::body::BoxBody>, Response = http::Response<<T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody>>,
<T as tonic::codegen::Service<http::Request<tonic::body::BoxBody>>>::Error: Into<StdError> + Send + Sync,
{
TextServiceClient::new(InterceptedService::new(inner, interceptor))
}
/// Compress requests with the given encoding.
///
/// This requires the server to support it otherwise it might respond with an
/// error.
#[must_use]
pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
self.inner = self.inner.send_compressed(encoding);
self
}
/// Enable decompressing responses.
#[must_use]
pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
self.inner = self.inner.accept_compressed(encoding);
self
}
/// Limits the maximum size of a decoded message.
///
/// Default: `4MB`
#[must_use]
pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
self.inner = self.inner.max_decoding_message_size(limit);
self
}
/// Limits the maximum size of an encoded message.
///
/// Default: `usize::MAX`
#[must_use]
pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
self.inner = self.inner.max_encoding_message_size(limit);
self
}
/// Generates a response from the model given an input message.
pub async fn generate_text(&mut self, request: impl tonic::IntoRequest<super::GenerateTextRequest>) -> std::result::Result<tonic::Response<super::GenerateTextResponse>, tonic::Status> {
self.inner.ready().await.map_err(|e| tonic::Status::new(tonic::Code::Unknown, format!("Service was not ready: {}", e.into())))?;
let codec = tonic::codec::ProstCodec::default();
let path = http::uri::PathAndQuery::from_static("/google.ai.generativelanguage.v1beta2.TextService/GenerateText");
let mut req = request.into_request();
req.extensions_mut().insert(GrpcMethod::new("google.ai.generativelanguage.v1beta2.TextService", "GenerateText"));
self.inner.unary(req, path, codec).await
}
/// Generates an embedding from the model given an input message.
pub async fn embed_text(&mut self, request: impl tonic::IntoRequest<super::EmbedTextRequest>) -> std::result::Result<tonic::Response<super::EmbedTextResponse>, tonic::Status> {
self.inner.ready().await.map_err(|e| tonic::Status::new(tonic::Code::Unknown, format!("Service was not ready: {}", e.into())))?;
let codec = tonic::codec::ProstCodec::default();
let path = http::uri::PathAndQuery::from_static("/google.ai.generativelanguage.v1beta2.TextService/EmbedText");
let mut req = request.into_request();
req.extensions_mut().insert(GrpcMethod::new("google.ai.generativelanguage.v1beta2.TextService", "EmbedText"));
self.inner.unary(req, path, codec).await
}
}
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GenerateMessageRequest {
#[prost(string, tag = "1")]
pub model: ::prost::alloc::string::String,
#[prost(message, optional, tag = "2")]
pub prompt: ::core::option::Option<MessagePrompt>,
#[prost(float, optional, tag = "3")]
pub temperature: ::core::option::Option<f32>,
#[prost(int32, optional, tag = "4")]
pub candidate_count: ::core::option::Option<i32>,
#[prost(float, optional, tag = "5")]
pub top_p: ::core::option::Option<f32>,
#[prost(int32, optional, tag = "6")]
pub top_k: ::core::option::Option<i32>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GenerateMessageResponse {
#[prost(message, repeated, tag = "1")]
pub candidates: ::prost::alloc::vec::Vec<Message>,
#[prost(message, repeated, tag = "2")]
pub messages: ::prost::alloc::vec::Vec<Message>,
#[prost(message, repeated, tag = "3")]
pub filters: ::prost::alloc::vec::Vec<ContentFilter>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Message {
#[prost(string, tag = "1")]
pub author: ::prost::alloc::string::String,
#[prost(string, tag = "2")]
pub content: ::prost::alloc::string::String,
#[prost(message, optional, tag = "3")]
pub citation_metadata: ::core::option::Option<CitationMetadata>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct MessagePrompt {
#[prost(string, tag = "1")]
pub context: ::prost::alloc::string::String,
#[prost(message, repeated, tag = "2")]
pub examples: ::prost::alloc::vec::Vec<Example>,
#[prost(message, repeated, tag = "3")]
pub messages: ::prost::alloc::vec::Vec<Message>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Example {
#[prost(message, optional, tag = "1")]
pub input: ::core::option::Option<Message>,
#[prost(message, optional, tag = "2")]
pub output: ::core::option::Option<Message>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CountMessageTokensRequest {
#[prost(string, tag = "1")]
pub model: ::prost::alloc::string::String,
#[prost(message, optional, tag = "2")]
pub prompt: ::core::option::Option<MessagePrompt>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct CountMessageTokensResponse {
#[prost(int32, tag = "1")]
pub token_count: i32,
}
/// Generated client implementations.
pub mod discuss_service_client {
#![allow(unused_variables, dead_code, missing_docs, clippy::let_unit_value)]
use tonic::codegen::http::Uri;
use tonic::codegen::*;
/// An API for using Generative Language Models (GLMs) in dialog applications.
///
/// Also known as large language models (LLMs), this API provides models that
/// are trained for multi-turn dialog.
#[derive(Debug, Clone)]
pub struct DiscussServiceClient<T> {
inner: tonic::client::Grpc<T>,
}
impl<T> DiscussServiceClient<T>
where
T: tonic::client::GrpcService<tonic::body::BoxBody>,
T::Error: Into<StdError>,
T::ResponseBody: Body<Data = Bytes> + Send + 'static,
<T::ResponseBody as Body>::Error: Into<StdError> + Send,
{
pub fn new(inner: T) -> Self {
let inner = tonic::client::Grpc::new(inner);
Self { inner }
}
pub fn with_origin(inner: T, origin: Uri) -> Self {
let inner = tonic::client::Grpc::with_origin(inner, origin);
Self { inner }
}
pub fn with_interceptor<F>(inner: T, interceptor: F) -> DiscussServiceClient<InterceptedService<T, F>>
where
F: tonic::service::Interceptor,
T::ResponseBody: Default,
T: tonic::codegen::Service<http::Request<tonic::body::BoxBody>, Response = http::Response<<T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody>>,
<T as tonic::codegen::Service<http::Request<tonic::body::BoxBody>>>::Error: Into<StdError> + Send + Sync,
{
DiscussServiceClient::new(InterceptedService::new(inner, interceptor))
}
/// Compress requests with the given encoding.
///
/// This requires the server to support it otherwise it might respond with an
/// error.
#[must_use]
pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
self.inner = self.inner.send_compressed(encoding);
self
}
/// Enable decompressing responses.
#[must_use]
pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
self.inner = self.inner.accept_compressed(encoding);
self
}
/// Limits the maximum size of a decoded message.
///
/// Default: `4MB`
#[must_use]
pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
self.inner = self.inner.max_decoding_message_size(limit);
self
}
/// Limits the maximum size of an encoded message.
///
/// Default: `usize::MAX`
#[must_use]
pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
self.inner = self.inner.max_encoding_message_size(limit);
self
}
/// Generates a response from the model given an input `MessagePrompt`.
pub async fn generate_message(&mut self, request: impl tonic::IntoRequest<super::GenerateMessageRequest>) -> std::result::Result<tonic::Response<super::GenerateMessageResponse>, tonic::Status> {
self.inner.ready().await.map_err(|e| tonic::Status::new(tonic::Code::Unknown, format!("Service was not ready: {}", e.into())))?;
let codec = tonic::codec::ProstCodec::default();
let path = http::uri::PathAndQuery::from_static("/google.ai.generativelanguage.v1beta2.DiscussService/GenerateMessage");
let mut req = request.into_request();
req.extensions_mut().insert(GrpcMethod::new("google.ai.generativelanguage.v1beta2.DiscussService", "GenerateMessage"));
self.inner.unary(req, path, codec).await
}
/// Runs a model's tokenizer on a string and returns the token count.
pub async fn count_message_tokens(&mut self, request: impl tonic::IntoRequest<super::CountMessageTokensRequest>) -> std::result::Result<tonic::Response<super::CountMessageTokensResponse>, tonic::Status> {
self.inner.ready().await.map_err(|e| tonic::Status::new(tonic::Code::Unknown, format!("Service was not ready: {}", e.into())))?;
let codec = tonic::codec::ProstCodec::default();
let path = http::uri::PathAndQuery::from_static("/google.ai.generativelanguage.v1beta2.DiscussService/CountMessageTokens");
let mut req = request.into_request();
req.extensions_mut().insert(GrpcMethod::new("google.ai.generativelanguage.v1beta2.DiscussService", "CountMessageTokens"));
self.inner.unary(req, path, codec).await
}
}
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Model {
#[prost(string, tag = "1")]
pub name: ::prost::alloc::string::String,
#[prost(string, tag = "2")]
pub base_model_id: ::prost::alloc::string::String,
#[prost(string, tag = "3")]
pub version: ::prost::alloc::string::String,
#[prost(string, tag = "4")]
pub display_name: ::prost::alloc::string::String,
#[prost(string, tag = "5")]
pub description: ::prost::alloc::string::String,
#[prost(int32, tag = "6")]
pub input_token_limit: i32,
#[prost(int32, tag = "7")]
pub output_token_limit: i32,
#[prost(string, repeated, tag = "8")]
pub supported_generation_methods: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
#[prost(float, optional, tag = "9")]
pub temperature: ::core::option::Option<f32>,
#[prost(float, optional, tag = "10")]
pub top_p: ::core::option::Option<f32>,
#[prost(int32, optional, tag = "11")]
pub top_k: ::core::option::Option<i32>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetModelRequest {
#[prost(string, tag = "1")]
pub name: ::prost::alloc::string::String,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListModelsRequest {
#[prost(int32, tag = "2")]
pub page_size: i32,
#[prost(string, tag = "3")]
pub page_token: ::prost::alloc::string::String,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListModelsResponse {
#[prost(message, repeated, tag = "1")]
pub models: ::prost::alloc::vec::Vec<Model>,
#[prost(string, tag = "2")]
pub next_page_token: ::prost::alloc::string::String,
}
/// Generated client implementations.
pub mod model_service_client {
#![allow(unused_variables, dead_code, missing_docs, clippy::let_unit_value)]
use tonic::codegen::http::Uri;
use tonic::codegen::*;
/// Provides methods for getting metadata information about Generative Models.
#[derive(Debug, Clone)]
pub struct ModelServiceClient<T> {
inner: tonic::client::Grpc<T>,
}
impl<T> ModelServiceClient<T>
where
T: tonic::client::GrpcService<tonic::body::BoxBody>,
T::Error: Into<StdError>,
T::ResponseBody: Body<Data = Bytes> + Send + 'static,
<T::ResponseBody as Body>::Error: Into<StdError> + Send,
{
pub fn new(inner: T) -> Self {
let inner = tonic::client::Grpc::new(inner);
Self { inner }
}
pub fn with_origin(inner: T, origin: Uri) -> Self {
let inner = tonic::client::Grpc::with_origin(inner, origin);
Self { inner }
}
pub fn with_interceptor<F>(inner: T, interceptor: F) -> ModelServiceClient<InterceptedService<T, F>>
where
F: tonic::service::Interceptor,
T::ResponseBody: Default,
T: tonic::codegen::Service<http::Request<tonic::body::BoxBody>, Response = http::Response<<T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody>>,
<T as tonic::codegen::Service<http::Request<tonic::body::BoxBody>>>::Error: Into<StdError> + Send + Sync,
{
ModelServiceClient::new(InterceptedService::new(inner, interceptor))
}
/// Compress requests with the given encoding.
///
/// This requires the server to support it otherwise it might respond with an
/// error.
#[must_use]
pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
self.inner = self.inner.send_compressed(encoding);
self
}
/// Enable decompressing responses.
#[must_use]
pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
self.inner = self.inner.accept_compressed(encoding);
self
}
/// Limits the maximum size of a decoded message.
///
/// Default: `4MB`
#[must_use]
pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
self.inner = self.inner.max_decoding_message_size(limit);
self
}
/// Limits the maximum size of an encoded message.
///
/// Default: `usize::MAX`
#[must_use]
pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
self.inner = self.inner.max_encoding_message_size(limit);
self
}
/// Gets information about a specific Model.
pub async fn get_model(&mut self, request: impl tonic::IntoRequest<super::GetModelRequest>) -> std::result::Result<tonic::Response<super::Model>, tonic::Status> {
self.inner.ready().await.map_err(|e| tonic::Status::new(tonic::Code::Unknown, format!("Service was not ready: {}", e.into())))?;
let codec = tonic::codec::ProstCodec::default();
let path = http::uri::PathAndQuery::from_static("/google.ai.generativelanguage.v1beta2.ModelService/GetModel");
let mut req = request.into_request();
req.extensions_mut().insert(GrpcMethod::new("google.ai.generativelanguage.v1beta2.ModelService", "GetModel"));
self.inner.unary(req, path, codec).await
}
/// Lists models available through the API.
pub async fn list_models(&mut self, request: impl tonic::IntoRequest<super::ListModelsRequest>) -> std::result::Result<tonic::Response<super::ListModelsResponse>, tonic::Status> {
self.inner.ready().await.map_err(|e| tonic::Status::new(tonic::Code::Unknown, format!("Service was not ready: {}", e.into())))?;
let codec = tonic::codec::ProstCodec::default();
let path = http::uri::PathAndQuery::from_static("/google.ai.generativelanguage.v1beta2.ModelService/ListModels");
let mut req = request.into_request();
req.extensions_mut().insert(GrpcMethod::new("google.ai.generativelanguage.v1beta2.ModelService", "ListModels"));
self.inner.unary(req, path, codec).await
}
}
}
