// This file is @generated by prost-build.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CitationMetadata {
#[prost(message, repeated, tag = "1")]
pub citation_sources: ::prost::alloc::vec::Vec<CitationSource>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CitationSource {
#[prost(int32, optional, tag = "1")]
pub start_index: ::core::option::Option<i32>,
#[prost(int32, optional, tag = "2")]
pub end_index: ::core::option::Option<i32>,
#[prost(string, optional, tag = "3")]
pub uri: ::core::option::Option<::prost::alloc::string::String>,
#[prost(string, optional, tag = "4")]
pub license: ::core::option::Option<::prost::alloc::string::String>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ContentFilter {
#[prost(enumeration = "content_filter::BlockedReason", tag = "1")]
pub reason: i32,
#[prost(string, optional, tag = "2")]
pub message: ::core::option::Option<::prost::alloc::string::String>,
}
/// Nested message and enum types in `ContentFilter`.
pub mod content_filter {
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum BlockedReason {
Unspecified = 0,
Safety = 1,
Other = 2,
}
impl BlockedReason {
/// String value of the enum field names used in the ProtoBuf definition.
///
/// The values are not transformed in any way and thus are considered stable
/// (if the ProtoBuf definition does not change) and safe for programmatic use.
pub fn as_str_name(&self) -> &'static str {
match self {
BlockedReason::Unspecified => "BLOCKED_REASON_UNSPECIFIED",
BlockedReason::Safety => "SAFETY",
BlockedReason::Other => "OTHER",
}
}
/// Creates an enum from field names used in the ProtoBuf definition.
pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
match value {
"BLOCKED_REASON_UNSPECIFIED" => Some(Self::Unspecified),
"SAFETY" => Some(Self::Safety),
"OTHER" => Some(Self::Other),
_ => None,
}
}
}
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct SafetyFeedback {
#[prost(message, optional, tag = "1")]
pub rating: ::core::option::Option<SafetyRating>,
#[prost(message, optional, tag = "2")]
pub setting: ::core::option::Option<SafetySetting>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct SafetyRating {
#[prost(enumeration = "HarmCategory", tag = "3")]
pub category: i32,
#[prost(enumeration = "safety_rating::HarmProbability", tag = "4")]
pub probability: i32,
}
/// Nested message and enum types in `SafetyRating`.
pub mod safety_rating {
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum HarmProbability {
Unspecified = 0,
Negligible = 1,
Low = 2,
Medium = 3,
High = 4,
}
impl HarmProbability {
/// String value of the enum field names used in the ProtoBuf definition.
///
/// The values are not transformed in any way and thus are considered stable
/// (if the ProtoBuf definition does not change) and safe for programmatic use.
pub fn as_str_name(&self) -> &'static str {
match self {
HarmProbability::Unspecified => "HARM_PROBABILITY_UNSPECIFIED",
HarmProbability::Negligible => "NEGLIGIBLE",
HarmProbability::Low => "LOW",
HarmProbability::Medium => "MEDIUM",
HarmProbability::High => "HIGH",
}
}
/// Creates an enum from field names used in the ProtoBuf definition.
pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
match value {
"HARM_PROBABILITY_UNSPECIFIED" => Some(Self::Unspecified),
"NEGLIGIBLE" => Some(Self::Negligible),
"LOW" => Some(Self::Low),
"MEDIUM" => Some(Self::Medium),
"HIGH" => Some(Self::High),
_ => None,
}
}
}
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct SafetySetting {
#[prost(enumeration = "HarmCategory", tag = "3")]
pub category: i32,
#[prost(enumeration = "safety_setting::HarmBlockThreshold", tag = "4")]
pub threshold: i32,
}
/// Nested message and enum types in `SafetySetting`.
pub mod safety_setting {
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum HarmBlockThreshold {
Unspecified = 0,
BlockLowAndAbove = 1,
BlockMediumAndAbove = 2,
BlockOnlyHigh = 3,
BlockNone = 4,
}
impl HarmBlockThreshold {
/// String value of the enum field names used in the ProtoBuf definition.
///
/// The values are not transformed in any way and thus are considered stable
/// (if the ProtoBuf definition does not change) and safe for programmatic use.
pub fn as_str_name(&self) -> &'static str {
match self {
HarmBlockThreshold::Unspecified => "HARM_BLOCK_THRESHOLD_UNSPECIFIED",
HarmBlockThreshold::BlockLowAndAbove => "BLOCK_LOW_AND_ABOVE",
HarmBlockThreshold::BlockMediumAndAbove => "BLOCK_MEDIUM_AND_ABOVE",
HarmBlockThreshold::BlockOnlyHigh => "BLOCK_ONLY_HIGH",
HarmBlockThreshold::BlockNone => "BLOCK_NONE",
}
}
/// Creates an enum from field names used in the ProtoBuf definition.
pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
match value {
"HARM_BLOCK_THRESHOLD_UNSPECIFIED" => Some(Self::Unspecified),
"BLOCK_LOW_AND_ABOVE" => Some(Self::BlockLowAndAbove),
"BLOCK_MEDIUM_AND_ABOVE" => Some(Self::BlockMediumAndAbove),
"BLOCK_ONLY_HIGH" => Some(Self::BlockOnlyHigh),
"BLOCK_NONE" => Some(Self::BlockNone),
_ => None,
}
}
}
}
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum HarmCategory {
Unspecified = 0,
Derogatory = 1,
Toxicity = 2,
Violence = 3,
Sexual = 4,
Medical = 5,
Dangerous = 6,
}
impl HarmCategory {
/// String value of the enum field names used in the ProtoBuf definition.
///
/// The values are not transformed in any way and thus are considered stable
/// (if the ProtoBuf definition does not change) and safe for programmatic use.
pub fn as_str_name(&self) -> &'static str {
match self {
HarmCategory::Unspecified => "HARM_CATEGORY_UNSPECIFIED",
HarmCategory::Derogatory => "HARM_CATEGORY_DEROGATORY",
HarmCategory::Toxicity => "HARM_CATEGORY_TOXICITY",
HarmCategory::Violence => "HARM_CATEGORY_VIOLENCE",
HarmCategory::Sexual => "HARM_CATEGORY_SEXUAL",
HarmCategory::Medical => "HARM_CATEGORY_MEDICAL",
HarmCategory::Dangerous => "HARM_CATEGORY_DANGEROUS",
}
}
/// Creates an enum from field names used in the ProtoBuf definition.
pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
match value {
"HARM_CATEGORY_UNSPECIFIED" => Some(Self::Unspecified),
"HARM_CATEGORY_DEROGATORY" => Some(Self::Derogatory),
"HARM_CATEGORY_TOXICITY" => Some(Self::Toxicity),
"HARM_CATEGORY_VIOLENCE" => Some(Self::Violence),
"HARM_CATEGORY_SEXUAL" => Some(Self::Sexual),
"HARM_CATEGORY_MEDICAL" => Some(Self::Medical),
"HARM_CATEGORY_DANGEROUS" => Some(Self::Dangerous),
_ => None,
}
}
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GenerateTextRequest {
#[prost(string, tag = "1")]
pub model: ::prost::alloc::string::String,
#[prost(message, optional, tag = "2")]
pub prompt: ::core::option::Option<TextPrompt>,
#[prost(float, optional, tag = "3")]
pub temperature: ::core::option::Option<f32>,
#[prost(int32, optional, tag = "4")]
pub candidate_count: ::core::option::Option<i32>,
#[prost(int32, optional, tag = "5")]
pub max_output_tokens: ::core::option::Option<i32>,
#[prost(float, optional, tag = "6")]
pub top_p: ::core::option::Option<f32>,
#[prost(int32, optional, tag = "7")]
pub top_k: ::core::option::Option<i32>,
#[prost(message, repeated, tag = "8")]
pub safety_settings: ::prost::alloc::vec::Vec<SafetySetting>,
#[prost(string, repeated, tag = "9")]
pub stop_sequences: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GenerateTextResponse {
#[prost(message, repeated, tag = "1")]
pub candidates: ::prost::alloc::vec::Vec<TextCompletion>,
#[prost(message, repeated, tag = "3")]
pub filters: ::prost::alloc::vec::Vec<ContentFilter>,
#[prost(message, repeated, tag = "4")]
pub safety_feedback: ::prost::alloc::vec::Vec<SafetyFeedback>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TextPrompt {
#[prost(string, tag = "1")]
pub text: ::prost::alloc::string::String,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TextCompletion {
#[prost(string, tag = "1")]
pub output: ::prost::alloc::string::String,
#[prost(message, repeated, tag = "2")]
pub safety_ratings: ::prost::alloc::vec::Vec<SafetyRating>,
#[prost(message, optional, tag = "3")]
pub citation_metadata: ::core::option::Option<CitationMetadata>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct EmbedTextRequest {
#[prost(string, tag = "1")]
pub model: ::prost::alloc::string::String,
#[prost(string, tag = "2")]
pub text: ::prost::alloc::string::String,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct EmbedTextResponse {
#[prost(message, optional, tag = "1")]
pub embedding: ::core::option::Option<Embedding>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BatchEmbedTextRequest {
#[prost(string, tag = "1")]
pub model: ::prost::alloc::string::String,
#[prost(string, repeated, tag = "2")]
pub texts: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BatchEmbedTextResponse {
#[prost(message, repeated, tag = "1")]
pub embeddings: ::prost::alloc::vec::Vec<Embedding>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Embedding {
#[prost(float, repeated, tag = "1")]
pub value: ::prost::alloc::vec::Vec<f32>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CountTextTokensRequest {
#[prost(string, tag = "1")]
pub model: ::prost::alloc::string::String,
#[prost(message, optional, tag = "2")]
pub prompt: ::core::option::Option<TextPrompt>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct CountTextTokensResponse {
#[prost(int32, tag = "1")]
pub token_count: i32,
}
/// Generated client implementations.
pub mod text_service_client {
#![allow(unused_variables, dead_code, missing_docs, clippy::let_unit_value)]
use tonic::codegen::http::Uri;
use tonic::codegen::*;
/// API for using Generative Language Models (GLMs) trained to generate text.
///
/// Also known as Large Language Models (LLM)s, these generate text given an
/// input prompt from the user.
#[derive(Debug, Clone)]
pub struct TextServiceClient<T> {
inner: tonic::client::Grpc<T>,
}
impl<T> TextServiceClient<T>
where
T: tonic::client::GrpcService<tonic::body::BoxBody>,
T::Error: Into<StdError>,
T::ResponseBody: Body<Data = Bytes> + Send + 'static,
<T::ResponseBody as Body>::Error: Into<StdError> + Send,
{
pub fn new(inner: T) -> Self {
let inner = tonic::client::Grpc::new(inner);
Self { inner }
}
pub fn with_origin(inner: T, origin: Uri) -> Self {
let inner = tonic::client::Grpc::with_origin(inner, origin);
Self { inner }
}
pub fn with_interceptor<F>(inner: T, interceptor: F) -> TextServiceClient<InterceptedService<T, F>>
where
F: tonic::service::Interceptor,
T::ResponseBody: Default,
T: tonic::codegen::Service<http::Request<tonic::body::BoxBody>, Response = http::Response<<T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody>>,
<T as tonic::codegen::Service<http::Request<tonic::body::BoxBody>>>::Error: Into<StdError> + Send + Sync,
{
TextServiceClient::new(InterceptedService::new(inner, interceptor))
}
/// Compress requests with the given encoding.
///
/// This requires the server to support it otherwise it might respond with an
/// error.
#[must_use]
pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
self.inner = self.inner.send_compressed(encoding);
self
}
/// Enable decompressing responses.
#[must_use]
pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
self.inner = self.inner.accept_compressed(encoding);
self
}
/// Limits the maximum size of a decoded message.
///
/// Default: `4MB`
#[must_use]
pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
self.inner = self.inner.max_decoding_message_size(limit);
self
}
/// Limits the maximum size of an encoded message.
///
/// Default: `usize::MAX`
#[must_use]
pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
self.inner = self.inner.max_encoding_message_size(limit);
self
}
/// Generates a response from the model given an input message.
pub async fn generate_text(&mut self, request: impl tonic::IntoRequest<super::GenerateTextRequest>) -> std::result::Result<tonic::Response<super::GenerateTextResponse>, tonic::Status> {
self.inner.ready().await.map_err(|e| tonic::Status::new(tonic::Code::Unknown, format!("Service was not ready: {}", e.into())))?;
let codec = tonic::codec::ProstCodec::default();
let path = http::uri::PathAndQuery::from_static("/google.ai.generativelanguage.v1beta3.TextService/GenerateText");
let mut req = request.into_request();
req.extensions_mut().insert(GrpcMethod::new("google.ai.generativelanguage.v1beta3.TextService", "GenerateText"));
self.inner.unary(req, path, codec).await
}
/// Generates an embedding from the model given an input message.
pub async fn embed_text(&mut self, request: impl tonic::IntoRequest<super::EmbedTextRequest>) -> std::result::Result<tonic::Response<super::EmbedTextResponse>, tonic::Status> {
self.inner.ready().await.map_err(|e| tonic::Status::new(tonic::Code::Unknown, format!("Service was not ready: {}", e.into())))?;
let codec = tonic::codec::ProstCodec::default();
let path = http::uri::PathAndQuery::from_static("/google.ai.generativelanguage.v1beta3.TextService/EmbedText");
let mut req = request.into_request();
req.extensions_mut().insert(GrpcMethod::new("google.ai.generativelanguage.v1beta3.TextService", "EmbedText"));
self.inner.unary(req, path, codec).await
}
/// Generates multiple embeddings from the model given input text in a
/// synchronous call.
pub async fn batch_embed_text(&mut self, request: impl tonic::IntoRequest<super::BatchEmbedTextRequest>) -> std::result::Result<tonic::Response<super::BatchEmbedTextResponse>, tonic::Status> {
self.inner.ready().await.map_err(|e| tonic::Status::new(tonic::Code::Unknown, format!("Service was not ready: {}", e.into())))?;
let codec = tonic::codec::ProstCodec::default();
let path = http::uri::PathAndQuery::from_static("/google.ai.generativelanguage.v1beta3.TextService/BatchEmbedText");
let mut req = request.into_request();
req.extensions_mut().insert(GrpcMethod::new("google.ai.generativelanguage.v1beta3.TextService", "BatchEmbedText"));
self.inner.unary(req, path, codec).await
}
/// Runs a model's tokenizer on a text and returns the token count.
pub async fn count_text_tokens(&mut self, request: impl tonic::IntoRequest<super::CountTextTokensRequest>) -> std::result::Result<tonic::Response<super::CountTextTokensResponse>, tonic::Status> {
self.inner.ready().await.map_err(|e| tonic::Status::new(tonic::Code::Unknown, format!("Service was not ready: {}", e.into())))?;
let codec = tonic::codec::ProstCodec::default();
let path = http::uri::PathAndQuery::from_static("/google.ai.generativelanguage.v1beta3.TextService/CountTextTokens");
let mut req = request.into_request();
req.extensions_mut().insert(GrpcMethod::new("google.ai.generativelanguage.v1beta3.TextService", "CountTextTokens"));
self.inner.unary(req, path, codec).await
}
}
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Permission {
#[prost(string, tag = "1")]
pub name: ::prost::alloc::string::String,
#[prost(enumeration = "permission::GranteeType", optional, tag = "2")]
pub grantee_type: ::core::option::Option<i32>,
#[prost(string, optional, tag = "3")]
pub email_address: ::core::option::Option<::prost::alloc::string::String>,
#[prost(enumeration = "permission::Role", optional, tag = "4")]
pub role: ::core::option::Option<i32>,
}
/// Nested message and enum types in `Permission`.
pub mod permission {
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum GranteeType {
Unspecified = 0,
User = 1,
Group = 2,
Everyone = 3,
}
impl GranteeType {
/// String value of the enum field names used in the ProtoBuf definition.
///
/// The values are not transformed in any way and thus are considered stable
/// (if the ProtoBuf definition does not change) and safe for programmatic use.
pub fn as_str_name(&self) -> &'static str {
match self {
GranteeType::Unspecified => "GRANTEE_TYPE_UNSPECIFIED",
GranteeType::User => "USER",
GranteeType::Group => "GROUP",
GranteeType::Everyone => "EVERYONE",
}
}
/// Creates an enum from field names used in the ProtoBuf definition.
pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
match value {
"GRANTEE_TYPE_UNSPECIFIED" => Some(Self::Unspecified),
"USER" => Some(Self::User),
"GROUP" => Some(Self::Group),
"EVERYONE" => Some(Self::Everyone),
_ => None,
}
}
}
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum Role {
Unspecified = 0,
Owner = 1,
Writer = 2,
Reader = 3,
}
impl Role {
/// String value of the enum field names used in the ProtoBuf definition.
///
/// The values are not transformed in any way and thus are considered stable
/// (if the ProtoBuf definition does not change) and safe for programmatic use.
pub fn as_str_name(&self) -> &'static str {
match self {
Role::Unspecified => "ROLE_UNSPECIFIED",
Role::Owner => "OWNER",
Role::Writer => "WRITER",
Role::Reader => "READER",
}
}
/// Creates an enum from field names used in the ProtoBuf definition.
pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
match value {
"ROLE_UNSPECIFIED" => Some(Self::Unspecified),
"OWNER" => Some(Self::Owner),
"WRITER" => Some(Self::Writer),
"READER" => Some(Self::Reader),
_ => None,
}
}
}
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GenerateMessageRequest {
#[prost(string, tag = "1")]
pub model: ::prost::alloc::string::String,
#[prost(message, optional, tag = "2")]
pub prompt: ::core::option::Option<MessagePrompt>,
#[prost(float, optional, tag = "3")]
pub temperature: ::core::option::Option<f32>,
#[prost(int32, optional, tag = "4")]
pub candidate_count: ::core::option::Option<i32>,
#[prost(float, optional, tag = "5")]
pub top_p: ::core::option::Option<f32>,
#[prost(int32, optional, tag = "6")]
pub top_k: ::core::option::Option<i32>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GenerateMessageResponse {
#[prost(message, repeated, tag = "1")]
pub candidates: ::prost::alloc::vec::Vec<Message>,
#[prost(message, repeated, tag = "2")]
pub messages: ::prost::alloc::vec::Vec<Message>,
#[prost(message, repeated, tag = "3")]
pub filters: ::prost::alloc::vec::Vec<ContentFilter>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Message {
#[prost(string, tag = "1")]
pub author: ::prost::alloc::string::String,
#[prost(string, tag = "2")]
pub content: ::prost::alloc::string::String,
#[prost(message, optional, tag = "3")]
pub citation_metadata: ::core::option::Option<CitationMetadata>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct MessagePrompt {
#[prost(string, tag = "1")]
pub context: ::prost::alloc::string::String,
#[prost(message, repeated, tag = "2")]
pub examples: ::prost::alloc::vec::Vec<Example>,
#[prost(message, repeated, tag = "3")]
pub messages: ::prost::alloc::vec::Vec<Message>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Example {
#[prost(message, optional, tag = "1")]
pub input: ::core::option::Option<Message>,
#[prost(message, optional, tag = "2")]
pub output: ::core::option::Option<Message>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CountMessageTokensRequest {
#[prost(string, tag = "1")]
pub model: ::prost::alloc::string::String,
#[prost(message, optional, tag = "2")]
pub prompt: ::core::option::Option<MessagePrompt>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct CountMessageTokensResponse {
#[prost(int32, tag = "1")]
pub token_count: i32,
}
/// Generated client implementations.
pub mod discuss_service_client {
#![allow(unused_variables, dead_code, missing_docs, clippy::let_unit_value)]
use tonic::codegen::http::Uri;
use tonic::codegen::*;
/// An API for using Generative Language Models (GLMs) in dialog applications.
///
/// Also known as large language models (LLMs), this API provides models that
/// are trained for multi-turn dialog.
#[derive(Debug, Clone)]
pub struct DiscussServiceClient<T> {
inner: tonic::client::Grpc<T>,
}
impl<T> DiscussServiceClient<T>
where
T: tonic::client::GrpcService<tonic::body::BoxBody>,
T::Error: Into<StdError>,
T::ResponseBody: Body<Data = Bytes> + Send + 'static,
<T::ResponseBody as Body>::Error: Into<StdError> + Send,
{
pub fn new(inner: T) -> Self {
let inner = tonic::client::Grpc::new(inner);
Self { inner }
}
pub fn with_origin(inner: T, origin: Uri) -> Self {
let inner = tonic::client::Grpc::with_origin(inner, origin);
Self { inner }
}
pub fn with_interceptor<F>(inner: T, interceptor: F) -> DiscussServiceClient<InterceptedService<T, F>>
where
F: tonic::service::Interceptor,
T::ResponseBody: Default,
T: tonic::codegen::Service<http::Request<tonic::body::BoxBody>, Response = http::Response<<T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody>>,
<T as tonic::codegen::Service<http::Request<tonic::body::BoxBody>>>::Error: Into<StdError> + Send + Sync,
{
DiscussServiceClient::new(InterceptedService::new(inner, interceptor))
}
/// Compress requests with the given encoding.
///
/// This requires the server to support it otherwise it might respond with an
/// error.
#[must_use]
pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
self.inner = self.inner.send_compressed(encoding);
self
}
/// Enable decompressing responses.
#[must_use]
pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
self.inner = self.inner.accept_compressed(encoding);
self
}
/// Limits the maximum size of a decoded message.
///
/// Default: `4MB`
#[must_use]
pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
self.inner = self.inner.max_decoding_message_size(limit);
self
}
/// Limits the maximum size of an encoded message.
///
/// Default: `usize::MAX`
#[must_use]
pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
self.inner = self.inner.max_encoding_message_size(limit);
self
}
/// Generates a response from the model given an input `MessagePrompt`.
pub async fn generate_message(&mut self, request: impl tonic::IntoRequest<super::GenerateMessageRequest>) -> std::result::Result<tonic::Response<super::GenerateMessageResponse>, tonic::Status> {
self.inner.ready().await.map_err(|e| tonic::Status::new(tonic::Code::Unknown, format!("Service was not ready: {}", e.into())))?;
let codec = tonic::codec::ProstCodec::default();
let path = http::uri::PathAndQuery::from_static("/google.ai.generativelanguage.v1beta3.DiscussService/GenerateMessage");
let mut req = request.into_request();
req.extensions_mut().insert(GrpcMethod::new("google.ai.generativelanguage.v1beta3.DiscussService", "GenerateMessage"));
self.inner.unary(req, path, codec).await
}
/// Runs a model's tokenizer on a string and returns the token count.
pub async fn count_message_tokens(&mut self, request: impl tonic::IntoRequest<super::CountMessageTokensRequest>) -> std::result::Result<tonic::Response<super::CountMessageTokensResponse>, tonic::Status> {
self.inner.ready().await.map_err(|e| tonic::Status::new(tonic::Code::Unknown, format!("Service was not ready: {}", e.into())))?;
let codec = tonic::codec::ProstCodec::default();
let path = http::uri::PathAndQuery::from_static("/google.ai.generativelanguage.v1beta3.DiscussService/CountMessageTokens");
let mut req = request.into_request();
req.extensions_mut().insert(GrpcMethod::new("google.ai.generativelanguage.v1beta3.DiscussService", "CountMessageTokens"));
self.inner.unary(req, path, codec).await
}
}
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TunedModel {
#[prost(string, tag = "1")]
pub name: ::prost::alloc::string::String,
#[prost(string, tag = "5")]
pub display_name: ::prost::alloc::string::String,
#[prost(string, tag = "6")]
pub description: ::prost::alloc::string::String,
#[prost(float, optional, tag = "11")]
pub temperature: ::core::option::Option<f32>,
#[prost(float, optional, tag = "12")]
pub top_p: ::core::option::Option<f32>,
#[prost(int32, optional, tag = "13")]
pub top_k: ::core::option::Option<i32>,
#[prost(enumeration = "tuned_model::State", tag = "7")]
pub state: i32,
#[prost(message, optional, tag = "8")]
pub create_time: ::core::option::Option<::prost_types::Timestamp>,
#[prost(message, optional, tag = "9")]
pub update_time: ::core::option::Option<::prost_types::Timestamp>,
#[prost(message, optional, tag = "10")]
pub tuning_task: ::core::option::Option<TuningTask>,
#[prost(oneof = "tuned_model::SourceModel", tags = "3, 4")]
pub source_model: ::core::option::Option<tuned_model::SourceModel>,
}
/// Nested message and enum types in `TunedModel`.
pub mod tuned_model {
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum State {
Unspecified = 0,
Creating = 1,
Active = 2,
Failed = 3,
}
impl State {
/// String value of the enum field names used in the ProtoBuf definition.
///
/// The values are not transformed in any way and thus are considered stable
/// (if the ProtoBuf definition does not change) and safe for programmatic use.
pub fn as_str_name(&self) -> &'static str {
match self {
State::Unspecified => "STATE_UNSPECIFIED",
State::Creating => "CREATING",
State::Active => "ACTIVE",
State::Failed => "FAILED",
}
}
/// Creates an enum from field names used in the ProtoBuf definition.
pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
match value {
"STATE_UNSPECIFIED" => Some(Self::Unspecified),
"CREATING" => Some(Self::Creating),
"ACTIVE" => Some(Self::Active),
"FAILED" => Some(Self::Failed),
_ => None,
}
}
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Oneof)]
pub enum SourceModel {
#[prost(message, tag = "3")]
TunedModelSource(super::TunedModelSource),
#[prost(string, tag = "4")]
BaseModel(::prost::alloc::string::String),
}
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TunedModelSource {
#[prost(string, tag = "1")]
pub tuned_model: ::prost::alloc::string::String,
#[prost(string, tag = "2")]
pub base_model: ::prost::alloc::string::String,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TuningTask {
#[prost(message, optional, tag = "1")]
pub start_time: ::core::option::Option<::prost_types::Timestamp>,
#[prost(message, optional, tag = "2")]
pub complete_time: ::core::option::Option<::prost_types::Timestamp>,
#[prost(message, repeated, tag = "3")]
pub snapshots: ::prost::alloc::vec::Vec<TuningSnapshot>,
#[prost(message, optional, tag = "4")]
pub training_data: ::core::option::Option<Dataset>,
#[prost(message, optional, tag = "5")]
pub hyperparameters: ::core::option::Option<Hyperparameters>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct Hyperparameters {
#[prost(int32, optional, tag = "14")]
pub epoch_count: ::core::option::Option<i32>,
#[prost(int32, optional, tag = "15")]
pub batch_size: ::core::option::Option<i32>,
#[prost(float, optional, tag = "16")]
pub learning_rate: ::core::option::Option<f32>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Dataset {
#[prost(oneof = "dataset::Dataset", tags = "1")]
pub dataset: ::core::option::Option<dataset::Dataset>,
}
/// Nested message and enum types in `Dataset`.
pub mod dataset {
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Oneof)]
pub enum Dataset {
#[prost(message, tag = "1")]
Examples(super::TuningExamples),
}
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TuningExamples {
#[prost(message, repeated, tag = "1")]
pub examples: ::prost::alloc::vec::Vec<TuningExample>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TuningExample {
#[prost(string, tag = "3")]
pub output: ::prost::alloc::string::String,
#[prost(oneof = "tuning_example::ModelInput", tags = "1")]
pub model_input: ::core::option::Option<tuning_example::ModelInput>,
}
/// Nested message and enum types in `TuningExample`.
pub mod tuning_example {
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Oneof)]
pub enum ModelInput {
#[prost(string, tag = "1")]
TextInput(::prost::alloc::string::String),
}
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct TuningSnapshot {
#[prost(int32, tag = "1")]
pub step: i32,
#[prost(int32, tag = "2")]
pub epoch: i32,
#[prost(float, tag = "3")]
pub mean_loss: f32,
#[prost(message, optional, tag = "4")]
pub compute_time: ::core::option::Option<::prost_types::Timestamp>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Model {
#[prost(string, tag = "1")]
pub name: ::prost::alloc::string::String,
#[prost(string, tag = "2")]
pub base_model_id: ::prost::alloc::string::String,
#[prost(string, tag = "3")]
pub version: ::prost::alloc::string::String,
#[prost(string, tag = "4")]
pub display_name: ::prost::alloc::string::String,
#[prost(string, tag = "5")]
pub description: ::prost::alloc::string::String,
#[prost(int32, tag = "6")]
pub input_token_limit: i32,
#[prost(int32, tag = "7")]
pub output_token_limit: i32,
#[prost(string, repeated, tag = "8")]
pub supported_generation_methods: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
#[prost(float, optional, tag = "9")]
pub temperature: ::core::option::Option<f32>,
#[prost(float, optional, tag = "10")]
pub top_p: ::core::option::Option<f32>,
#[prost(int32, optional, tag = "11")]
pub top_k: ::core::option::Option<i32>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreatePermissionRequest {
#[prost(string, tag = "1")]
pub parent: ::prost::alloc::string::String,
#[prost(message, optional, tag = "2")]
pub permission: ::core::option::Option<Permission>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetPermissionRequest {
#[prost(string, tag = "1")]
pub name: ::prost::alloc::string::String,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListPermissionsRequest {
#[prost(string, tag = "1")]
pub parent: ::prost::alloc::string::String,
#[prost(int32, tag = "2")]
pub page_size: i32,
#[prost(string, tag = "3")]
pub page_token: ::prost::alloc::string::String,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListPermissionsResponse {
#[prost(message, repeated, tag = "1")]
pub permissions: ::prost::alloc::vec::Vec<Permission>,
#[prost(string, tag = "2")]
pub next_page_token: ::prost::alloc::string::String,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdatePermissionRequest {
#[prost(message, optional, tag = "1")]
pub permission: ::core::option::Option<Permission>,
#[prost(message, optional, tag = "2")]
pub update_mask: ::core::option::Option<::prost_types::FieldMask>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeletePermissionRequest {
#[prost(string, tag = "1")]
pub name: ::prost::alloc::string::String,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TransferOwnershipRequest {
#[prost(string, tag = "1")]
pub name: ::prost::alloc::string::String,
#[prost(string, tag = "2")]
pub email_address: ::prost::alloc::string::String,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct TransferOwnershipResponse {}
/// Generated client implementations.
pub mod permission_service_client {
#![allow(unused_variables, dead_code, missing_docs, clippy::let_unit_value)]
use tonic::codegen::http::Uri;
use tonic::codegen::*;
/// Provides methods for managing permissions to PaLM API resources.
#[derive(Debug, Clone)]
pub struct PermissionServiceClient<T> {
inner: tonic::client::Grpc<T>,
}
impl<T> PermissionServiceClient<T>
where
T: tonic::client::GrpcService<tonic::body::BoxBody>,
T::Error: Into<StdError>,
T::ResponseBody: Body<Data = Bytes> + Send + 'static,
<T::ResponseBody as Body>::Error: Into<StdError> + Send,
{
pub fn new(inner: T) -> Self {
let inner = tonic::client::Grpc::new(inner);
Self { inner }
}
pub fn with_origin(inner: T, origin: Uri) -> Self {
let inner = tonic::client::Grpc::with_origin(inner, origin);
Self { inner }
}
pub fn with_interceptor<F>(inner: T, interceptor: F) -> PermissionServiceClient<InterceptedService<T, F>>
where
F: tonic::service::Interceptor,
T::ResponseBody: Default,
T: tonic::codegen::Service<http::Request<tonic::body::BoxBody>, Response = http::Response<<T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody>>,
<T as tonic::codegen::Service<http::Request<tonic::body::BoxBody>>>::Error: Into<StdError> + Send + Sync,
{
PermissionServiceClient::new(InterceptedService::new(inner, interceptor))
}
/// Compress requests with the given encoding.
///
/// This requires the server to support it otherwise it might respond with an
/// error.
#[must_use]
pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
self.inner = self.inner.send_compressed(encoding);
self
}
/// Enable decompressing responses.
#[must_use]
pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
self.inner = self.inner.accept_compressed(encoding);
self
}
/// Limits the maximum size of a decoded message.
///
/// Default: `4MB`
#[must_use]
pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
self.inner = self.inner.max_decoding_message_size(limit);
self
}
/// Limits the maximum size of an encoded message.
///
/// Default: `usize::MAX`
#[must_use]
pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
self.inner = self.inner.max_encoding_message_size(limit);
self
}
/// Create a permission to a specific resource.
pub async fn create_permission(&mut self, request: impl tonic::IntoRequest<super::CreatePermissionRequest>) -> std::result::Result<tonic::Response<super::Permission>, tonic::Status> {
self.inner.ready().await.map_err(|e| tonic::Status::new(tonic::Code::Unknown, format!("Service was not ready: {}", e.into())))?;
let codec = tonic::codec::ProstCodec::default();
let path = http::uri::PathAndQuery::from_static("/google.ai.generativelanguage.v1beta3.PermissionService/CreatePermission");
let mut req = request.into_request();
req.extensions_mut().insert(GrpcMethod::new("google.ai.generativelanguage.v1beta3.PermissionService", "CreatePermission"));
self.inner.unary(req, path, codec).await
}
/// Gets information about a specific Permission.
pub async fn get_permission(&mut self, request: impl tonic::IntoRequest<super::GetPermissionRequest>) -> std::result::Result<tonic::Response<super::Permission>, tonic::Status> {
self.inner.ready().await.map_err(|e| tonic::Status::new(tonic::Code::Unknown, format!("Service was not ready: {}", e.into())))?;
let codec = tonic::codec::ProstCodec::default();
let path = http::uri::PathAndQuery::from_static("/google.ai.generativelanguage.v1beta3.PermissionService/GetPermission");
let mut req = request.into_request();
req.extensions_mut().insert(GrpcMethod::new("google.ai.generativelanguage.v1beta3.PermissionService", "GetPermission"));
self.inner.unary(req, path, codec).await
}
/// Lists permissions for the specific resource.
pub async fn list_permissions(&mut self, request: impl tonic::IntoRequest<super::ListPermissionsRequest>) -> std::result::Result<tonic::Response<super::ListPermissionsResponse>, tonic::Status> {
self.inner.ready().await.map_err(|e| tonic::Status::new(tonic::Code::Unknown, format!("Service was not ready: {}", e.into())))?;
let codec = tonic::codec::ProstCodec::default();
let path = http::uri::PathAndQuery::from_static("/google.ai.generativelanguage.v1beta3.PermissionService/ListPermissions");
let mut req = request.into_request();
req.extensions_mut().insert(GrpcMethod::new("google.ai.generativelanguage.v1beta3.PermissionService", "ListPermissions"));
self.inner.unary(req, path, codec).await
}
/// Updates the permission.
pub async fn update_permission(&mut self, request: impl tonic::IntoRequest<super::UpdatePermissionRequest>) -> std::result::Result<tonic::Response<super::Permission>, tonic::Status> {
self.inner.ready().await.map_err(|e| tonic::Status::new(tonic::Code::Unknown, format!("Service was not ready: {}", e.into())))?;
let codec = tonic::codec::ProstCodec::default();
let path = http::uri::PathAndQuery::from_static("/google.ai.generativelanguage.v1beta3.PermissionService/UpdatePermission");
let mut req = request.into_request();
req.extensions_mut().insert(GrpcMethod::new("google.ai.generativelanguage.v1beta3.PermissionService", "UpdatePermission"));
self.inner.unary(req, path, codec).await
}
/// Deletes the permission.
pub async fn delete_permission(&mut self, request: impl tonic::IntoRequest<super::DeletePermissionRequest>) -> std::result::Result<tonic::Response<()>, tonic::Status> {
self.inner.ready().await.map_err(|e| tonic::Status::new(tonic::Code::Unknown, format!("Service was not ready: {}", e.into())))?;
let codec = tonic::codec::ProstCodec::default();
let path = http::uri::PathAndQuery::from_static("/google.ai.generativelanguage.v1beta3.PermissionService/DeletePermission");
let mut req = request.into_request();
req.extensions_mut().insert(GrpcMethod::new("google.ai.generativelanguage.v1beta3.PermissionService", "DeletePermission"));
self.inner.unary(req, path, codec).await
}
/// Transfers ownership of the tuned model.
/// This is the only way to change ownership of the tuned model.
/// The current owner will be downgraded to writer role.
pub async fn transfer_ownership(&mut self, request: impl tonic::IntoRequest<super::TransferOwnershipRequest>) -> std::result::Result<tonic::Response<super::TransferOwnershipResponse>, tonic::Status> {
self.inner.ready().await.map_err(|e| tonic::Status::new(tonic::Code::Unknown, format!("Service was not ready: {}", e.into())))?;
let codec = tonic::codec::ProstCodec::default();
let path = http::uri::PathAndQuery::from_static("/google.ai.generativelanguage.v1beta3.PermissionService/TransferOwnership");
let mut req = request.into_request();
req.extensions_mut().insert(GrpcMethod::new("google.ai.generativelanguage.v1beta3.PermissionService", "TransferOwnership"));
self.inner.unary(req, path, codec).await
}
}
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetModelRequest {
#[prost(string, tag = "1")]
pub name: ::prost::alloc::string::String,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListModelsRequest {
#[prost(int32, tag = "2")]
pub page_size: i32,
#[prost(string, tag = "3")]
pub page_token: ::prost::alloc::string::String,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListModelsResponse {
#[prost(message, repeated, tag = "1")]
pub models: ::prost::alloc::vec::Vec<Model>,
#[prost(string, tag = "2")]
pub next_page_token: ::prost::alloc::string::String,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetTunedModelRequest {
#[prost(string, tag = "1")]
pub name: ::prost::alloc::string::String,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListTunedModelsRequest {
#[prost(int32, tag = "1")]
pub page_size: i32,
#[prost(string, tag = "2")]
pub page_token: ::prost::alloc::string::String,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListTunedModelsResponse {
#[prost(message, repeated, tag = "1")]
pub tuned_models: ::prost::alloc::vec::Vec<TunedModel>,
#[prost(string, tag = "2")]
pub next_page_token: ::prost::alloc::string::String,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateTunedModelRequest {
#[prost(string, optional, tag = "1")]
pub tuned_model_id: ::core::option::Option<::prost::alloc::string::String>,
#[prost(message, optional, tag = "2")]
pub tuned_model: ::core::option::Option<TunedModel>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateTunedModelMetadata {
#[prost(string, tag = "5")]
pub tuned_model: ::prost::alloc::string::String,
#[prost(int32, tag = "1")]
pub total_steps: i32,
#[prost(int32, tag = "2")]
pub completed_steps: i32,
#[prost(float, tag = "3")]
pub completed_percent: f32,
#[prost(message, repeated, tag = "4")]
pub snapshots: ::prost::alloc::vec::Vec<TuningSnapshot>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateTunedModelRequest {
#[prost(message, optional, tag = "1")]
pub tuned_model: ::core::option::Option<TunedModel>,
#[prost(message, optional, tag = "2")]
pub update_mask: ::core::option::Option<::prost_types::FieldMask>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteTunedModelRequest {
#[prost(string, tag = "1")]
pub name: ::prost::alloc::string::String,
}
/// Generated client implementations.
pub mod model_service_client {
#![allow(unused_variables, dead_code, missing_docs, clippy::let_unit_value)]
use tonic::codegen::http::Uri;
use tonic::codegen::*;
/// Provides methods for getting metadata information about Generative Models.
#[derive(Debug, Clone)]
pub struct ModelServiceClient<T> {
inner: tonic::client::Grpc<T>,
}
impl<T> ModelServiceClient<T>
where
T: tonic::client::GrpcService<tonic::body::BoxBody>,
T::Error: Into<StdError>,
T::ResponseBody: Body<Data = Bytes> + Send + 'static,
<T::ResponseBody as Body>::Error: Into<StdError> + Send,
{
pub fn new(inner: T) -> Self {
let inner = tonic::client::Grpc::new(inner);
Self { inner }
}
pub fn with_origin(inner: T, origin: Uri) -> Self {
let inner = tonic::client::Grpc::with_origin(inner, origin);
Self { inner }
}
pub fn with_interceptor<F>(inner: T, interceptor: F) -> ModelServiceClient<InterceptedService<T, F>>
where
F: tonic::service::Interceptor,
T::ResponseBody: Default,
T: tonic::codegen::Service<http::Request<tonic::body::BoxBody>, Response = http::Response<<T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody>>,
<T as tonic::codegen::Service<http::Request<tonic::body::BoxBody>>>::Error: Into<StdError> + Send + Sync,
{
ModelServiceClient::new(InterceptedService::new(inner, interceptor))
}
/// Compress requests with the given encoding.
///
/// This requires the server to support it otherwise it might respond with an
/// error.
#[must_use]
pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
self.inner = self.inner.send_compressed(encoding);
self
}
/// Enable decompressing responses.
#[must_use]
pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
self.inner = self.inner.accept_compressed(encoding);
self
}
/// Limits the maximum size of a decoded message.
///
/// Default: `4MB`
#[must_use]
pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
self.inner = self.inner.max_decoding_message_size(limit);
self
}
/// Limits the maximum size of an encoded message.
///
/// Default: `usize::MAX`
#[must_use]
pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
self.inner = self.inner.max_encoding_message_size(limit);
self
}
/// Gets information about a specific Model.
pub async fn get_model(&mut self, request: impl tonic::IntoRequest<super::GetModelRequest>) -> std::result::Result<tonic::Response<super::Model>, tonic::Status> {
self.inner.ready().await.map_err(|e| tonic::Status::new(tonic::Code::Unknown, format!("Service was not ready: {}", e.into())))?;
let codec = tonic::codec::ProstCodec::default();
let path = http::uri::PathAndQuery::from_static("/google.ai.generativelanguage.v1beta3.ModelService/GetModel");
let mut req = request.into_request();
req.extensions_mut().insert(GrpcMethod::new("google.ai.generativelanguage.v1beta3.ModelService", "GetModel"));
self.inner.unary(req, path, codec).await
}
/// Lists models available through the API.
pub async fn list_models(&mut self, request: impl tonic::IntoRequest<super::ListModelsRequest>) -> std::result::Result<tonic::Response<super::ListModelsResponse>, tonic::Status> {
self.inner.ready().await.map_err(|e| tonic::Status::new(tonic::Code::Unknown, format!("Service was not ready: {}", e.into())))?;
let codec = tonic::codec::ProstCodec::default();
let path = http::uri::PathAndQuery::from_static("/google.ai.generativelanguage.v1beta3.ModelService/ListModels");
let mut req = request.into_request();
req.extensions_mut().insert(GrpcMethod::new("google.ai.generativelanguage.v1beta3.ModelService", "ListModels"));
self.inner.unary(req, path, codec).await
}
/// Gets information about a specific TunedModel.
pub async fn get_tuned_model(&mut self, request: impl tonic::IntoRequest<super::GetTunedModelRequest>) -> std::result::Result<tonic::Response<super::TunedModel>, tonic::Status> {
self.inner.ready().await.map_err(|e| tonic::Status::new(tonic::Code::Unknown, format!("Service was not ready: {}", e.into())))?;
let codec = tonic::codec::ProstCodec::default();
let path = http::uri::PathAndQuery::from_static("/google.ai.generativelanguage.v1beta3.ModelService/GetTunedModel");
let mut req = request.into_request();
req.extensions_mut().insert(GrpcMethod::new("google.ai.generativelanguage.v1beta3.ModelService", "GetTunedModel"));
self.inner.unary(req, path, codec).await
}
/// Lists tuned models owned by the user.
pub async fn list_tuned_models(&mut self, request: impl tonic::IntoRequest<super::ListTunedModelsRequest>) -> std::result::Result<tonic::Response<super::ListTunedModelsResponse>, tonic::Status> {
self.inner.ready().await.map_err(|e| tonic::Status::new(tonic::Code::Unknown, format!("Service was not ready: {}", e.into())))?;
let codec = tonic::codec::ProstCodec::default();
let path = http::uri::PathAndQuery::from_static("/google.ai.generativelanguage.v1beta3.ModelService/ListTunedModels");
let mut req = request.into_request();
req.extensions_mut().insert(GrpcMethod::new("google.ai.generativelanguage.v1beta3.ModelService", "ListTunedModels"));
self.inner.unary(req, path, codec).await
}
/// Creates a tuned model.
/// Intermediate tuning progress (if any) is accessed through the
/// [google.longrunning.Operations] service.
///
/// Status and results can be accessed through the Operations service.
/// Example:
///   GET /v1/tunedModels/az2mb0bpw6i/operations/000-111-222
pub async fn create_tuned_model(&mut self, request: impl tonic::IntoRequest<super::CreateTunedModelRequest>) -> std::result::Result<tonic::Response<super::super::super::super::longrunning::Operation>, tonic::Status> {
self.inner.ready().await.map_err(|e| tonic::Status::new(tonic::Code::Unknown, format!("Service was not ready: {}", e.into())))?;
let codec = tonic::codec::ProstCodec::default();
let path = http::uri::PathAndQuery::from_static("/google.ai.generativelanguage.v1beta3.ModelService/CreateTunedModel");
let mut req = request.into_request();
req.extensions_mut().insert(GrpcMethod::new("google.ai.generativelanguage.v1beta3.ModelService", "CreateTunedModel"));
self.inner.unary(req, path, codec).await
}
/// Updates a tuned model.
pub async fn update_tuned_model(&mut self, request: impl tonic::IntoRequest<super::UpdateTunedModelRequest>) -> std::result::Result<tonic::Response<super::TunedModel>, tonic::Status> {
self.inner.ready().await.map_err(|e| tonic::Status::new(tonic::Code::Unknown, format!("Service was not ready: {}", e.into())))?;
let codec = tonic::codec::ProstCodec::default();
let path = http::uri::PathAndQuery::from_static("/google.ai.generativelanguage.v1beta3.ModelService/UpdateTunedModel");
let mut req = request.into_request();
req.extensions_mut().insert(GrpcMethod::new("google.ai.generativelanguage.v1beta3.ModelService", "UpdateTunedModel"));
self.inner.unary(req, path, codec).await
}
/// Deletes a tuned model.
pub async fn delete_tuned_model(&mut self, request: impl tonic::IntoRequest<super::DeleteTunedModelRequest>) -> std::result::Result<tonic::Response<()>, tonic::Status> {
self.inner.ready().await.map_err(|e| tonic::Status::new(tonic::Code::Unknown, format!("Service was not ready: {}", e.into())))?;
let codec = tonic::codec::ProstCodec::default();
let path = http::uri::PathAndQuery::from_static("/google.ai.generativelanguage.v1beta3.ModelService/DeleteTunedModel");
let mut req = request.into_request();
req.extensions_mut().insert(GrpcMethod::new("google.ai.generativelanguage.v1beta3.ModelService", "DeleteTunedModel"));
self.inner.unary(req, path, codec).await
}
}
}
