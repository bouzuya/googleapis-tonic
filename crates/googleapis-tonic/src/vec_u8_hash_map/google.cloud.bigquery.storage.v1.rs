// This file is @generated by prost-build.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ProtoSchema {
    #[prost(message, optional, tag = "1")]
    pub proto_descriptor: ::core::option::Option<::prost_types::DescriptorProto>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ProtoRows {
    #[prost(bytes = "vec", repeated, tag = "1")]
    pub serialized_rows: ::prost::alloc::vec::Vec<::prost::alloc::vec::Vec<u8>>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ArrowSchema {
    #[prost(bytes = "vec", tag = "1")]
    pub serialized_schema: ::prost::alloc::vec::Vec<u8>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ArrowRecordBatch {
    #[prost(bytes = "vec", tag = "1")]
    pub serialized_record_batch: ::prost::alloc::vec::Vec<u8>,
    #[deprecated]
    #[prost(int64, tag = "2")]
    pub row_count: i64,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct ArrowSerializationOptions {
    #[prost(
        enumeration = "arrow_serialization_options::CompressionCodec",
        tag = "2"
    )]
    pub buffer_compression: i32,
}
/// Nested message and enum types in `ArrowSerializationOptions`.
pub mod arrow_serialization_options {
    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
    #[repr(i32)]
    pub enum CompressionCodec {
        CompressionUnspecified = 0,
        Lz4Frame = 1,
        Zstd = 2,
    }
    impl CompressionCodec {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                CompressionCodec::CompressionUnspecified => "COMPRESSION_UNSPECIFIED",
                CompressionCodec::Lz4Frame => "LZ4_FRAME",
                CompressionCodec::Zstd => "ZSTD",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "COMPRESSION_UNSPECIFIED" => Some(Self::CompressionUnspecified),
                "LZ4_FRAME" => Some(Self::Lz4Frame),
                "ZSTD" => Some(Self::Zstd),
                _ => None,
            }
        }
    }
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct AvroSchema {
    #[prost(string, tag = "1")]
    pub schema: ::prost::alloc::string::String,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct AvroRows {
    #[prost(bytes = "vec", tag = "1")]
    pub serialized_binary_rows: ::prost::alloc::vec::Vec<u8>,
    #[deprecated]
    #[prost(int64, tag = "2")]
    pub row_count: i64,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct AvroSerializationOptions {
    #[prost(bool, tag = "1")]
    pub enable_display_name_attribute: bool,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TableSchema {
    #[prost(message, repeated, tag = "1")]
    pub fields: ::prost::alloc::vec::Vec<TableFieldSchema>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TableFieldSchema {
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    #[prost(enumeration = "table_field_schema::Type", tag = "2")]
    pub r#type: i32,
    #[prost(enumeration = "table_field_schema::Mode", tag = "3")]
    pub mode: i32,
    #[prost(message, repeated, tag = "4")]
    pub fields: ::prost::alloc::vec::Vec<TableFieldSchema>,
    #[prost(string, tag = "6")]
    pub description: ::prost::alloc::string::String,
    #[prost(int64, tag = "7")]
    pub max_length: i64,
    #[prost(int64, tag = "8")]
    pub precision: i64,
    #[prost(int64, tag = "9")]
    pub scale: i64,
    #[prost(string, tag = "10")]
    pub default_value_expression: ::prost::alloc::string::String,
    #[prost(message, optional, tag = "11")]
    pub range_element_type: ::core::option::Option<table_field_schema::FieldElementType>,
}
/// Nested message and enum types in `TableFieldSchema`.
pub mod table_field_schema {
    #[allow(clippy::derive_partial_eq_without_eq)]
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct FieldElementType {
        #[prost(enumeration = "Type", tag = "1")]
        pub r#type: i32,
    }
    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
    #[repr(i32)]
    pub enum Type {
        Unspecified = 0,
        String = 1,
        Int64 = 2,
        Double = 3,
        Struct = 4,
        Bytes = 5,
        Bool = 6,
        Timestamp = 7,
        Date = 8,
        Time = 9,
        Datetime = 10,
        Geography = 11,
        Numeric = 12,
        Bignumeric = 13,
        Interval = 14,
        Json = 15,
        Range = 16,
    }
    impl Type {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Type::Unspecified => "TYPE_UNSPECIFIED",
                Type::String => "STRING",
                Type::Int64 => "INT64",
                Type::Double => "DOUBLE",
                Type::Struct => "STRUCT",
                Type::Bytes => "BYTES",
                Type::Bool => "BOOL",
                Type::Timestamp => "TIMESTAMP",
                Type::Date => "DATE",
                Type::Time => "TIME",
                Type::Datetime => "DATETIME",
                Type::Geography => "GEOGRAPHY",
                Type::Numeric => "NUMERIC",
                Type::Bignumeric => "BIGNUMERIC",
                Type::Interval => "INTERVAL",
                Type::Json => "JSON",
                Type::Range => "RANGE",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "TYPE_UNSPECIFIED" => Some(Self::Unspecified),
                "STRING" => Some(Self::String),
                "INT64" => Some(Self::Int64),
                "DOUBLE" => Some(Self::Double),
                "STRUCT" => Some(Self::Struct),
                "BYTES" => Some(Self::Bytes),
                "BOOL" => Some(Self::Bool),
                "TIMESTAMP" => Some(Self::Timestamp),
                "DATE" => Some(Self::Date),
                "TIME" => Some(Self::Time),
                "DATETIME" => Some(Self::Datetime),
                "GEOGRAPHY" => Some(Self::Geography),
                "NUMERIC" => Some(Self::Numeric),
                "BIGNUMERIC" => Some(Self::Bignumeric),
                "INTERVAL" => Some(Self::Interval),
                "JSON" => Some(Self::Json),
                "RANGE" => Some(Self::Range),
                _ => None,
            }
        }
    }
    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
    #[repr(i32)]
    pub enum Mode {
        Unspecified = 0,
        Nullable = 1,
        Required = 2,
        Repeated = 3,
    }
    impl Mode {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Mode::Unspecified => "MODE_UNSPECIFIED",
                Mode::Nullable => "NULLABLE",
                Mode::Required => "REQUIRED",
                Mode::Repeated => "REPEATED",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "MODE_UNSPECIFIED" => Some(Self::Unspecified),
                "NULLABLE" => Some(Self::Nullable),
                "REQUIRED" => Some(Self::Required),
                "REPEATED" => Some(Self::Repeated),
                _ => None,
            }
        }
    }
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ReadSession {
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    #[prost(message, optional, tag = "2")]
    pub expire_time: ::core::option::Option<::prost_types::Timestamp>,
    #[prost(enumeration = "DataFormat", tag = "3")]
    pub data_format: i32,
    #[prost(string, tag = "6")]
    pub table: ::prost::alloc::string::String,
    #[prost(message, optional, tag = "7")]
    pub table_modifiers: ::core::option::Option<read_session::TableModifiers>,
    #[prost(message, optional, tag = "8")]
    pub read_options: ::core::option::Option<read_session::TableReadOptions>,
    #[prost(message, repeated, tag = "10")]
    pub streams: ::prost::alloc::vec::Vec<ReadStream>,
    #[prost(int64, tag = "12")]
    pub estimated_total_bytes_scanned: i64,
    #[prost(int64, tag = "15")]
    pub estimated_total_physical_file_size: i64,
    #[prost(int64, tag = "14")]
    pub estimated_row_count: i64,
    #[prost(string, tag = "13")]
    pub trace_id: ::prost::alloc::string::String,
    #[prost(oneof = "read_session::Schema", tags = "4, 5")]
    pub schema: ::core::option::Option<read_session::Schema>,
}
/// Nested message and enum types in `ReadSession`.
pub mod read_session {
    #[allow(clippy::derive_partial_eq_without_eq)]
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct TableModifiers {
        #[prost(message, optional, tag = "1")]
        pub snapshot_time: ::core::option::Option<::prost_types::Timestamp>,
    }
    #[allow(clippy::derive_partial_eq_without_eq)]
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct TableReadOptions {
        #[prost(string, repeated, tag = "1")]
        pub selected_fields: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
        #[prost(string, tag = "2")]
        pub row_restriction: ::prost::alloc::string::String,
        #[prost(double, optional, tag = "5")]
        pub sample_percentage: ::core::option::Option<f64>,
        #[prost(
            enumeration = "table_read_options::ResponseCompressionCodec",
            optional,
            tag = "6"
        )]
        pub response_compression_codec: ::core::option::Option<i32>,
        #[prost(
            oneof = "table_read_options::OutputFormatSerializationOptions",
            tags = "3, 4"
        )]
        pub output_format_serialization_options:
            ::core::option::Option<table_read_options::OutputFormatSerializationOptions>,
    }
    /// Nested message and enum types in `TableReadOptions`.
    pub mod table_read_options {
        #[derive(
            Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration,
        )]
        #[repr(i32)]
        pub enum ResponseCompressionCodec {
            Unspecified = 0,
            Lz4 = 2,
        }
        impl ResponseCompressionCodec {
            /// String value of the enum field names used in the ProtoBuf definition.
            ///
            /// The values are not transformed in any way and thus are considered stable
            /// (if the ProtoBuf definition does not change) and safe for programmatic use.
            pub fn as_str_name(&self) -> &'static str {
                match self {
                    ResponseCompressionCodec::Unspecified => {
                        "RESPONSE_COMPRESSION_CODEC_UNSPECIFIED"
                    }
                    ResponseCompressionCodec::Lz4 => "RESPONSE_COMPRESSION_CODEC_LZ4",
                }
            }
            /// Creates an enum from field names used in the ProtoBuf definition.
            pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
                match value {
                    "RESPONSE_COMPRESSION_CODEC_UNSPECIFIED" => Some(Self::Unspecified),
                    "RESPONSE_COMPRESSION_CODEC_LZ4" => Some(Self::Lz4),
                    _ => None,
                }
            }
        }
        #[allow(clippy::derive_partial_eq_without_eq)]
        #[derive(Clone, Copy, PartialEq, ::prost::Oneof)]
        pub enum OutputFormatSerializationOptions {
            #[prost(message, tag = "3")]
            ArrowSerializationOptions(super::super::ArrowSerializationOptions),
            #[prost(message, tag = "4")]
            AvroSerializationOptions(super::super::AvroSerializationOptions),
        }
    }
    #[allow(clippy::derive_partial_eq_without_eq)]
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Schema {
        #[prost(message, tag = "4")]
        AvroSchema(super::AvroSchema),
        #[prost(message, tag = "5")]
        ArrowSchema(super::ArrowSchema),
    }
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ReadStream {
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct WriteStream {
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    #[prost(enumeration = "write_stream::Type", tag = "2")]
    pub r#type: i32,
    #[prost(message, optional, tag = "3")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    #[prost(message, optional, tag = "4")]
    pub commit_time: ::core::option::Option<::prost_types::Timestamp>,
    #[prost(message, optional, tag = "5")]
    pub table_schema: ::core::option::Option<TableSchema>,
    #[prost(enumeration = "write_stream::WriteMode", tag = "7")]
    pub write_mode: i32,
    #[prost(string, tag = "8")]
    pub location: ::prost::alloc::string::String,
}
/// Nested message and enum types in `WriteStream`.
pub mod write_stream {
    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
    #[repr(i32)]
    pub enum Type {
        Unspecified = 0,
        Committed = 1,
        Pending = 2,
        Buffered = 3,
    }
    impl Type {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Type::Unspecified => "TYPE_UNSPECIFIED",
                Type::Committed => "COMMITTED",
                Type::Pending => "PENDING",
                Type::Buffered => "BUFFERED",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "TYPE_UNSPECIFIED" => Some(Self::Unspecified),
                "COMMITTED" => Some(Self::Committed),
                "PENDING" => Some(Self::Pending),
                "BUFFERED" => Some(Self::Buffered),
                _ => None,
            }
        }
    }
    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
    #[repr(i32)]
    pub enum WriteMode {
        Unspecified = 0,
        Insert = 1,
    }
    impl WriteMode {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                WriteMode::Unspecified => "WRITE_MODE_UNSPECIFIED",
                WriteMode::Insert => "INSERT",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "WRITE_MODE_UNSPECIFIED" => Some(Self::Unspecified),
                "INSERT" => Some(Self::Insert),
                _ => None,
            }
        }
    }
}
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum DataFormat {
    Unspecified = 0,
    Avro = 1,
    Arrow = 2,
}
impl DataFormat {
    /// String value of the enum field names used in the ProtoBuf definition.
    ///
    /// The values are not transformed in any way and thus are considered stable
    /// (if the ProtoBuf definition does not change) and safe for programmatic use.
    pub fn as_str_name(&self) -> &'static str {
        match self {
            DataFormat::Unspecified => "DATA_FORMAT_UNSPECIFIED",
            DataFormat::Avro => "AVRO",
            DataFormat::Arrow => "ARROW",
        }
    }
    /// Creates an enum from field names used in the ProtoBuf definition.
    pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
        match value {
            "DATA_FORMAT_UNSPECIFIED" => Some(Self::Unspecified),
            "AVRO" => Some(Self::Avro),
            "ARROW" => Some(Self::Arrow),
            _ => None,
        }
    }
}
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum WriteStreamView {
    Unspecified = 0,
    Basic = 1,
    Full = 2,
}
impl WriteStreamView {
    /// String value of the enum field names used in the ProtoBuf definition.
    ///
    /// The values are not transformed in any way and thus are considered stable
    /// (if the ProtoBuf definition does not change) and safe for programmatic use.
    pub fn as_str_name(&self) -> &'static str {
        match self {
            WriteStreamView::Unspecified => "WRITE_STREAM_VIEW_UNSPECIFIED",
            WriteStreamView::Basic => "BASIC",
            WriteStreamView::Full => "FULL",
        }
    }
    /// Creates an enum from field names used in the ProtoBuf definition.
    pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
        match value {
            "WRITE_STREAM_VIEW_UNSPECIFIED" => Some(Self::Unspecified),
            "BASIC" => Some(Self::Basic),
            "FULL" => Some(Self::Full),
            _ => None,
        }
    }
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateReadSessionRequest {
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    #[prost(message, optional, tag = "2")]
    pub read_session: ::core::option::Option<ReadSession>,
    #[prost(int32, tag = "3")]
    pub max_stream_count: i32,
    #[prost(int32, tag = "4")]
    pub preferred_min_stream_count: i32,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ReadRowsRequest {
    #[prost(string, tag = "1")]
    pub read_stream: ::prost::alloc::string::String,
    #[prost(int64, tag = "2")]
    pub offset: i64,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct ThrottleState {
    #[prost(int32, tag = "1")]
    pub throttle_percent: i32,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct StreamStats {
    #[prost(message, optional, tag = "2")]
    pub progress: ::core::option::Option<stream_stats::Progress>,
}
/// Nested message and enum types in `StreamStats`.
pub mod stream_stats {
    #[allow(clippy::derive_partial_eq_without_eq)]
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct Progress {
        #[prost(double, tag = "1")]
        pub at_response_start: f64,
        #[prost(double, tag = "2")]
        pub at_response_end: f64,
    }
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ReadRowsResponse {
    #[prost(int64, tag = "6")]
    pub row_count: i64,
    #[prost(message, optional, tag = "2")]
    pub stats: ::core::option::Option<StreamStats>,
    #[prost(message, optional, tag = "5")]
    pub throttle_state: ::core::option::Option<ThrottleState>,
    #[prost(int64, optional, tag = "9")]
    pub uncompressed_byte_size: ::core::option::Option<i64>,
    #[prost(oneof = "read_rows_response::Rows", tags = "3, 4")]
    pub rows: ::core::option::Option<read_rows_response::Rows>,
    #[prost(oneof = "read_rows_response::Schema", tags = "7, 8")]
    pub schema: ::core::option::Option<read_rows_response::Schema>,
}
/// Nested message and enum types in `ReadRowsResponse`.
pub mod read_rows_response {
    #[allow(clippy::derive_partial_eq_without_eq)]
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Rows {
        #[prost(message, tag = "3")]
        AvroRows(super::AvroRows),
        #[prost(message, tag = "4")]
        ArrowRecordBatch(super::ArrowRecordBatch),
    }
    #[allow(clippy::derive_partial_eq_without_eq)]
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Schema {
        #[prost(message, tag = "7")]
        AvroSchema(super::AvroSchema),
        #[prost(message, tag = "8")]
        ArrowSchema(super::ArrowSchema),
    }
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SplitReadStreamRequest {
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    #[prost(double, tag = "2")]
    pub fraction: f64,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SplitReadStreamResponse {
    #[prost(message, optional, tag = "1")]
    pub primary_stream: ::core::option::Option<ReadStream>,
    #[prost(message, optional, tag = "2")]
    pub remainder_stream: ::core::option::Option<ReadStream>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateWriteStreamRequest {
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    #[prost(message, optional, tag = "2")]
    pub write_stream: ::core::option::Option<WriteStream>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct AppendRowsRequest {
    #[prost(string, tag = "1")]
    pub write_stream: ::prost::alloc::string::String,
    #[prost(message, optional, tag = "2")]
    pub offset: ::core::option::Option<i64>,
    #[prost(string, tag = "6")]
    pub trace_id: ::prost::alloc::string::String,
    #[prost(
        map = "string, enumeration(append_rows_request::MissingValueInterpretation)",
        tag = "7"
    )]
    pub missing_value_interpretations:
        ::std::collections::HashMap<::prost::alloc::string::String, i32>,
    #[prost(
        enumeration = "append_rows_request::MissingValueInterpretation",
        tag = "8"
    )]
    pub default_missing_value_interpretation: i32,
    #[prost(oneof = "append_rows_request::Rows", tags = "4")]
    pub rows: ::core::option::Option<append_rows_request::Rows>,
}
/// Nested message and enum types in `AppendRowsRequest`.
pub mod append_rows_request {
    #[allow(clippy::derive_partial_eq_without_eq)]
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct ProtoData {
        #[prost(message, optional, tag = "1")]
        pub writer_schema: ::core::option::Option<super::ProtoSchema>,
        #[prost(message, optional, tag = "2")]
        pub rows: ::core::option::Option<super::ProtoRows>,
    }
    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
    #[repr(i32)]
    pub enum MissingValueInterpretation {
        Unspecified = 0,
        NullValue = 1,
        DefaultValue = 2,
    }
    impl MissingValueInterpretation {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                MissingValueInterpretation::Unspecified => {
                    "MISSING_VALUE_INTERPRETATION_UNSPECIFIED"
                }
                MissingValueInterpretation::NullValue => "NULL_VALUE",
                MissingValueInterpretation::DefaultValue => "DEFAULT_VALUE",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "MISSING_VALUE_INTERPRETATION_UNSPECIFIED" => Some(Self::Unspecified),
                "NULL_VALUE" => Some(Self::NullValue),
                "DEFAULT_VALUE" => Some(Self::DefaultValue),
                _ => None,
            }
        }
    }
    #[allow(clippy::derive_partial_eq_without_eq)]
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Rows {
        #[prost(message, tag = "4")]
        ProtoRows(ProtoData),
    }
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct AppendRowsResponse {
    #[prost(message, optional, tag = "3")]
    pub updated_schema: ::core::option::Option<TableSchema>,
    #[prost(message, repeated, tag = "4")]
    pub row_errors: ::prost::alloc::vec::Vec<RowError>,
    #[prost(string, tag = "5")]
    pub write_stream: ::prost::alloc::string::String,
    #[prost(oneof = "append_rows_response::Response", tags = "1, 2")]
    pub response: ::core::option::Option<append_rows_response::Response>,
}
/// Nested message and enum types in `AppendRowsResponse`.
pub mod append_rows_response {
    #[allow(clippy::derive_partial_eq_without_eq)]
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct AppendResult {
        #[prost(message, optional, tag = "1")]
        pub offset: ::core::option::Option<i64>,
    }
    #[allow(clippy::derive_partial_eq_without_eq)]
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Response {
        #[prost(message, tag = "1")]
        AppendResult(AppendResult),
        #[prost(message, tag = "2")]
        Error(super::super::super::super::super::rpc::Status),
    }
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetWriteStreamRequest {
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    #[prost(enumeration = "WriteStreamView", tag = "3")]
    pub view: i32,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BatchCommitWriteStreamsRequest {
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    #[prost(string, repeated, tag = "2")]
    pub write_streams: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BatchCommitWriteStreamsResponse {
    #[prost(message, optional, tag = "1")]
    pub commit_time: ::core::option::Option<::prost_types::Timestamp>,
    #[prost(message, repeated, tag = "2")]
    pub stream_errors: ::prost::alloc::vec::Vec<StorageError>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct FinalizeWriteStreamRequest {
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct FinalizeWriteStreamResponse {
    #[prost(int64, tag = "1")]
    pub row_count: i64,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct FlushRowsRequest {
    #[prost(string, tag = "1")]
    pub write_stream: ::prost::alloc::string::String,
    #[prost(message, optional, tag = "2")]
    pub offset: ::core::option::Option<i64>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct FlushRowsResponse {
    #[prost(int64, tag = "1")]
    pub offset: i64,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct StorageError {
    #[prost(enumeration = "storage_error::StorageErrorCode", tag = "1")]
    pub code: i32,
    #[prost(string, tag = "2")]
    pub entity: ::prost::alloc::string::String,
    #[prost(string, tag = "3")]
    pub error_message: ::prost::alloc::string::String,
}
/// Nested message and enum types in `StorageError`.
pub mod storage_error {
    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
    #[repr(i32)]
    pub enum StorageErrorCode {
        Unspecified = 0,
        TableNotFound = 1,
        StreamAlreadyCommitted = 2,
        StreamNotFound = 3,
        InvalidStreamType = 4,
        InvalidStreamState = 5,
        StreamFinalized = 6,
        SchemaMismatchExtraFields = 7,
        OffsetAlreadyExists = 8,
        OffsetOutOfRange = 9,
        CmekNotProvided = 10,
        InvalidCmekProvided = 11,
        CmekEncryptionError = 12,
        KmsServiceError = 13,
        KmsPermissionDenied = 14,
    }
    impl StorageErrorCode {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                StorageErrorCode::Unspecified => "STORAGE_ERROR_CODE_UNSPECIFIED",
                StorageErrorCode::TableNotFound => "TABLE_NOT_FOUND",
                StorageErrorCode::StreamAlreadyCommitted => "STREAM_ALREADY_COMMITTED",
                StorageErrorCode::StreamNotFound => "STREAM_NOT_FOUND",
                StorageErrorCode::InvalidStreamType => "INVALID_STREAM_TYPE",
                StorageErrorCode::InvalidStreamState => "INVALID_STREAM_STATE",
                StorageErrorCode::StreamFinalized => "STREAM_FINALIZED",
                StorageErrorCode::SchemaMismatchExtraFields => "SCHEMA_MISMATCH_EXTRA_FIELDS",
                StorageErrorCode::OffsetAlreadyExists => "OFFSET_ALREADY_EXISTS",
                StorageErrorCode::OffsetOutOfRange => "OFFSET_OUT_OF_RANGE",
                StorageErrorCode::CmekNotProvided => "CMEK_NOT_PROVIDED",
                StorageErrorCode::InvalidCmekProvided => "INVALID_CMEK_PROVIDED",
                StorageErrorCode::CmekEncryptionError => "CMEK_ENCRYPTION_ERROR",
                StorageErrorCode::KmsServiceError => "KMS_SERVICE_ERROR",
                StorageErrorCode::KmsPermissionDenied => "KMS_PERMISSION_DENIED",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "STORAGE_ERROR_CODE_UNSPECIFIED" => Some(Self::Unspecified),
                "TABLE_NOT_FOUND" => Some(Self::TableNotFound),
                "STREAM_ALREADY_COMMITTED" => Some(Self::StreamAlreadyCommitted),
                "STREAM_NOT_FOUND" => Some(Self::StreamNotFound),
                "INVALID_STREAM_TYPE" => Some(Self::InvalidStreamType),
                "INVALID_STREAM_STATE" => Some(Self::InvalidStreamState),
                "STREAM_FINALIZED" => Some(Self::StreamFinalized),
                "SCHEMA_MISMATCH_EXTRA_FIELDS" => Some(Self::SchemaMismatchExtraFields),
                "OFFSET_ALREADY_EXISTS" => Some(Self::OffsetAlreadyExists),
                "OFFSET_OUT_OF_RANGE" => Some(Self::OffsetOutOfRange),
                "CMEK_NOT_PROVIDED" => Some(Self::CmekNotProvided),
                "INVALID_CMEK_PROVIDED" => Some(Self::InvalidCmekProvided),
                "CMEK_ENCRYPTION_ERROR" => Some(Self::CmekEncryptionError),
                "KMS_SERVICE_ERROR" => Some(Self::KmsServiceError),
                "KMS_PERMISSION_DENIED" => Some(Self::KmsPermissionDenied),
                _ => None,
            }
        }
    }
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct RowError {
    #[prost(int64, tag = "1")]
    pub index: i64,
    #[prost(enumeration = "row_error::RowErrorCode", tag = "2")]
    pub code: i32,
    #[prost(string, tag = "3")]
    pub message: ::prost::alloc::string::String,
}
/// Nested message and enum types in `RowError`.
pub mod row_error {
    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
    #[repr(i32)]
    pub enum RowErrorCode {
        Unspecified = 0,
        FieldsError = 1,
    }
    impl RowErrorCode {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                RowErrorCode::Unspecified => "ROW_ERROR_CODE_UNSPECIFIED",
                RowErrorCode::FieldsError => "FIELDS_ERROR",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "ROW_ERROR_CODE_UNSPECIFIED" => Some(Self::Unspecified),
                "FIELDS_ERROR" => Some(Self::FieldsError),
                _ => None,
            }
        }
    }
}
/// Generated client implementations.
pub mod big_query_read_client {
    #![allow(unused_variables, dead_code, missing_docs, clippy::let_unit_value)]
    use tonic::codegen::http::Uri;
    use tonic::codegen::*;
    /// BigQuery Read API.
    ///
    /// The Read API can be used to read data from BigQuery.
    #[derive(Debug, Clone)]
    pub struct BigQueryReadClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl<T> BigQueryReadClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> BigQueryReadClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<http::Request<tonic::body::BoxBody>>>::Error:
                Into<StdError> + Send + Sync,
        {
            BigQueryReadClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Creates a new read session. A read session divides the contents of a
        /// BigQuery table into one or more streams, which can then be used to read
        /// data from the table. The read session also specifies properties of the
        /// data to be read, such as a list of columns or a push-down filter describing
        /// the rows to be returned.
        ///
        /// A particular row can be read by at most one stream. When the caller has
        /// reached the end of each stream in the session, then all the data in the
        /// table has been read.
        ///
        /// Data is assigned to each stream such that roughly the same number of
        /// rows can be read from each stream. Because the server-side unit for
        /// assigning data is collections of rows, the API does not guarantee that
        /// each stream will return the same number or rows. Additionally, the
        /// limits are enforced based on the number of pre-filtered rows, so some
        /// filters can lead to lopsided assignments.
        ///
        /// Read sessions automatically expire 6 hours after they are created and do
        /// not require manual clean-up by the caller.
        pub async fn create_read_session(
            &mut self,
            request: impl tonic::IntoRequest<super::CreateReadSessionRequest>,
        ) -> std::result::Result<tonic::Response<super::ReadSession>, tonic::Status> {
            self.inner.ready().await.map_err(|e| {
                tonic::Status::new(
                    tonic::Code::Unknown,
                    format!("Service was not ready: {}", e.into()),
                )
            })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.bigquery.storage.v1.BigQueryRead/CreateReadSession",
            );
            let mut req = request.into_request();
            req.extensions_mut().insert(GrpcMethod::new(
                "google.cloud.bigquery.storage.v1.BigQueryRead",
                "CreateReadSession",
            ));
            self.inner.unary(req, path, codec).await
        }
        /// Reads rows from the stream in the format prescribed by the ReadSession.
        /// Each response contains one or more table rows, up to a maximum of 100 MiB
        /// per response; read requests which attempt to read individual rows larger
        /// than 100 MiB will fail.
        ///
        /// Each request also returns a set of stream statistics reflecting the current
        /// state of the stream.
        pub async fn read_rows(
            &mut self,
            request: impl tonic::IntoRequest<super::ReadRowsRequest>,
        ) -> std::result::Result<
            tonic::Response<tonic::codec::Streaming<super::ReadRowsResponse>>,
            tonic::Status,
        > {
            self.inner.ready().await.map_err(|e| {
                tonic::Status::new(
                    tonic::Code::Unknown,
                    format!("Service was not ready: {}", e.into()),
                )
            })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.bigquery.storage.v1.BigQueryRead/ReadRows",
            );
            let mut req = request.into_request();
            req.extensions_mut().insert(GrpcMethod::new(
                "google.cloud.bigquery.storage.v1.BigQueryRead",
                "ReadRows",
            ));
            self.inner.server_streaming(req, path, codec).await
        }
        /// Splits a given `ReadStream` into two `ReadStream` objects. These
        /// `ReadStream` objects are referred to as the primary and the residual
        /// streams of the split. The original `ReadStream` can still be read from in
        /// the same manner as before. Both of the returned `ReadStream` objects can
        /// also be read from, and the rows returned by both child streams will be
        /// the same as the rows read from the original stream.
        ///
        /// Moreover, the two child streams will be allocated back-to-back in the
        /// original `ReadStream`. Concretely, it is guaranteed that for streams
        /// original, primary, and residual, that original[0-j] = primary[0-j] and
        /// original[j-n] = residual[0-m] once the streams have been read to
        /// completion.
        pub async fn split_read_stream(
            &mut self,
            request: impl tonic::IntoRequest<super::SplitReadStreamRequest>,
        ) -> std::result::Result<tonic::Response<super::SplitReadStreamResponse>, tonic::Status>
        {
            self.inner.ready().await.map_err(|e| {
                tonic::Status::new(
                    tonic::Code::Unknown,
                    format!("Service was not ready: {}", e.into()),
                )
            })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.bigquery.storage.v1.BigQueryRead/SplitReadStream",
            );
            let mut req = request.into_request();
            req.extensions_mut().insert(GrpcMethod::new(
                "google.cloud.bigquery.storage.v1.BigQueryRead",
                "SplitReadStream",
            ));
            self.inner.unary(req, path, codec).await
        }
    }
}
/// Generated client implementations.
pub mod big_query_write_client {
    #![allow(unused_variables, dead_code, missing_docs, clippy::let_unit_value)]
    use tonic::codegen::http::Uri;
    use tonic::codegen::*;
    /// BigQuery Write API.
    ///
    /// The Write API can be used to write data to BigQuery.
    ///
    /// For supplementary information about the Write API, see:
    /// https://cloud.google.com/bigquery/docs/write-api
    #[derive(Debug, Clone)]
    pub struct BigQueryWriteClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl<T> BigQueryWriteClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> BigQueryWriteClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<http::Request<tonic::body::BoxBody>>>::Error:
                Into<StdError> + Send + Sync,
        {
            BigQueryWriteClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Creates a write stream to the given table.
        /// Additionally, every table has a special stream named '_default'
        /// to which data can be written. This stream doesn't need to be created using
        /// CreateWriteStream. It is a stream that can be used simultaneously by any
        /// number of clients. Data written to this stream is considered committed as
        /// soon as an acknowledgement is received.
        pub async fn create_write_stream(
            &mut self,
            request: impl tonic::IntoRequest<super::CreateWriteStreamRequest>,
        ) -> std::result::Result<tonic::Response<super::WriteStream>, tonic::Status> {
            self.inner.ready().await.map_err(|e| {
                tonic::Status::new(
                    tonic::Code::Unknown,
                    format!("Service was not ready: {}", e.into()),
                )
            })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.bigquery.storage.v1.BigQueryWrite/CreateWriteStream",
            );
            let mut req = request.into_request();
            req.extensions_mut().insert(GrpcMethod::new(
                "google.cloud.bigquery.storage.v1.BigQueryWrite",
                "CreateWriteStream",
            ));
            self.inner.unary(req, path, codec).await
        }
        /// Appends data to the given stream.
        ///
        /// If `offset` is specified, the `offset` is checked against the end of
        /// stream. The server returns `OUT_OF_RANGE` in `AppendRowsResponse` if an
        /// attempt is made to append to an offset beyond the current end of the stream
        /// or `ALREADY_EXISTS` if user provides an `offset` that has already been
        /// written to. User can retry with adjusted offset within the same RPC
        /// connection. If `offset` is not specified, append happens at the end of the
        /// stream.
        ///
        /// The response contains an optional offset at which the append
        /// happened.  No offset information will be returned for appends to a
        /// default stream.
        ///
        /// Responses are received in the same order in which requests are sent.
        /// There will be one response for each successful inserted request.  Responses
        /// may optionally embed error information if the originating AppendRequest was
        /// not successfully processed.
        ///
        /// The specifics of when successfully appended data is made visible to the
        /// table are governed by the type of stream:
        ///
        /// * For COMMITTED streams (which includes the default stream), data is
        /// visible immediately upon successful append.
        ///
        /// * For BUFFERED streams, data is made visible via a subsequent `FlushRows`
        /// rpc which advances a cursor to a newer offset in the stream.
        ///
        /// * For PENDING streams, data is not made visible until the stream itself is
        /// finalized (via the `FinalizeWriteStream` rpc), and the stream is explicitly
        /// committed via the `BatchCommitWriteStreams` rpc.
        pub async fn append_rows(
            &mut self,
            request: impl tonic::IntoStreamingRequest<Message = super::AppendRowsRequest>,
        ) -> std::result::Result<
            tonic::Response<tonic::codec::Streaming<super::AppendRowsResponse>>,
            tonic::Status,
        > {
            self.inner.ready().await.map_err(|e| {
                tonic::Status::new(
                    tonic::Code::Unknown,
                    format!("Service was not ready: {}", e.into()),
                )
            })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.bigquery.storage.v1.BigQueryWrite/AppendRows",
            );
            let mut req = request.into_streaming_request();
            req.extensions_mut().insert(GrpcMethod::new(
                "google.cloud.bigquery.storage.v1.BigQueryWrite",
                "AppendRows",
            ));
            self.inner.streaming(req, path, codec).await
        }
        /// Gets information about a write stream.
        pub async fn get_write_stream(
            &mut self,
            request: impl tonic::IntoRequest<super::GetWriteStreamRequest>,
        ) -> std::result::Result<tonic::Response<super::WriteStream>, tonic::Status> {
            self.inner.ready().await.map_err(|e| {
                tonic::Status::new(
                    tonic::Code::Unknown,
                    format!("Service was not ready: {}", e.into()),
                )
            })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.bigquery.storage.v1.BigQueryWrite/GetWriteStream",
            );
            let mut req = request.into_request();
            req.extensions_mut().insert(GrpcMethod::new(
                "google.cloud.bigquery.storage.v1.BigQueryWrite",
                "GetWriteStream",
            ));
            self.inner.unary(req, path, codec).await
        }
        /// Finalize a write stream so that no new data can be appended to the
        /// stream. Finalize is not supported on the '_default' stream.
        pub async fn finalize_write_stream(
            &mut self,
            request: impl tonic::IntoRequest<super::FinalizeWriteStreamRequest>,
        ) -> std::result::Result<tonic::Response<super::FinalizeWriteStreamResponse>, tonic::Status>
        {
            self.inner.ready().await.map_err(|e| {
                tonic::Status::new(
                    tonic::Code::Unknown,
                    format!("Service was not ready: {}", e.into()),
                )
            })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.bigquery.storage.v1.BigQueryWrite/FinalizeWriteStream",
            );
            let mut req = request.into_request();
            req.extensions_mut().insert(GrpcMethod::new(
                "google.cloud.bigquery.storage.v1.BigQueryWrite",
                "FinalizeWriteStream",
            ));
            self.inner.unary(req, path, codec).await
        }
        /// Atomically commits a group of `PENDING` streams that belong to the same
        /// `parent` table.
        ///
        /// Streams must be finalized before commit and cannot be committed multiple
        /// times. Once a stream is committed, data in the stream becomes available
        /// for read operations.
        pub async fn batch_commit_write_streams(
            &mut self,
            request: impl tonic::IntoRequest<super::BatchCommitWriteStreamsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::BatchCommitWriteStreamsResponse>,
            tonic::Status,
        > {
            self.inner.ready().await.map_err(|e| {
                tonic::Status::new(
                    tonic::Code::Unknown,
                    format!("Service was not ready: {}", e.into()),
                )
            })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.bigquery.storage.v1.BigQueryWrite/BatchCommitWriteStreams",
            );
            let mut req = request.into_request();
            req.extensions_mut().insert(GrpcMethod::new(
                "google.cloud.bigquery.storage.v1.BigQueryWrite",
                "BatchCommitWriteStreams",
            ));
            self.inner.unary(req, path, codec).await
        }
        /// Flushes rows to a BUFFERED stream.
        ///
        /// If users are appending rows to BUFFERED stream, flush operation is
        /// required in order for the rows to become available for reading. A
        /// Flush operation flushes up to any previously flushed offset in a BUFFERED
        /// stream, to the offset specified in the request.
        ///
        /// Flush is not supported on the _default stream, since it is not BUFFERED.
        pub async fn flush_rows(
            &mut self,
            request: impl tonic::IntoRequest<super::FlushRowsRequest>,
        ) -> std::result::Result<tonic::Response<super::FlushRowsResponse>, tonic::Status> {
            self.inner.ready().await.map_err(|e| {
                tonic::Status::new(
                    tonic::Code::Unknown,
                    format!("Service was not ready: {}", e.into()),
                )
            })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.bigquery.storage.v1.BigQueryWrite/FlushRows",
            );
            let mut req = request.into_request();
            req.extensions_mut().insert(GrpcMethod::new(
                "google.cloud.bigquery.storage.v1.BigQueryWrite",
                "FlushRows",
            ));
            self.inner.unary(req, path, codec).await
        }
    }
}
